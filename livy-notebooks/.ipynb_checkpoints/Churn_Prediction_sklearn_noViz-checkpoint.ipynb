{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction with IBM Db2 using Python\n",
    "\n",
    "__Introduction__\n",
    "\n",
    "This notebook presents a churn prediction use case using anonymized customer data from a phone operator. It uses IBM Db2 Warehouse and runs on a PySpark kernel. \n",
    "\n",
    "Our goal is to accurately predict whether a customer is going to end his/her contract (labeled as positive,1). We prefer to send a commercial email to someone who intends to keep her contract but is labeled as willing to end it (false positive) rather than to overlook the opportunity to prevent a customer from ending her contract (false negative), but also care not to overwhelm our customers with commercials or to lose money by proposing special offers to too many people. Our optimization objective will here be to maximize recall (tp/(tp+fn). We will also look at other classic metrics too.\n",
    "\n",
    "__Contents__\n",
    "1. Get ready\n",
    "2. Explore the data\n",
    "3. Prepare the data\n",
    "4. Build your model on the training set\n",
    "5. Fine tune your hyperparameters on the validation set\n",
    "6. Assess your model performance on the test set\n",
    "7. Save your model for future use\n",
    "\n",
    "\n",
    "__Requirements:__\n",
    "\n",
    "* PySpark kernel for your Jupyter notebook\n",
    "* Python 3 including pandas, sklearn, joblib \n",
    "* connection to Db2 Warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Imports__\n",
    "\n",
    "We import pandas for data exploration and transformation, sklearn for modelisation and evaluation, joblib for model persistence. We will also plot some graphs locally with matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>124</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%local\n",
    "%matplotlib inline\n",
    "\n",
    "# Useful imports for local visualisations\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load the data__\n",
    "\n",
    "Let's use a table which has been pre-populated in Db2 local. It is called SAMPLES.TRAINING. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+----------+--------+---------+----------+--------+---------+----------+----------+-----------+------------+---------+----------+-----------+---------+\n",
      "|CHURN|AREA|VMAIL|VMAIL_MSGS|DAY_MINS|DAY_CALLS|DAY_CHARGE|EVE_MINS|EVE_CALLS|EVE_CHARGE|NIGHT_MINS|NIGHT_CALLS|NIGHT_CHARGE|INTL_MINS|INTL_CALLS|INTL_CHARGE|SVC_CALLS|\n",
      "+-----+----+-----+----------+--------+---------+----------+--------+---------+----------+----------+-----------+------------+---------+----------+-----------+---------+\n",
      "|    0| 415|    1|         0|   246.5|      108|     41.91|   216.3|       89|     18.39|     179.6|         99|        8.08|     12.7|         3|       3.43|        2|\n",
      "|    1| 408|    1|         0|   298.1|      112|     50.68|   201.3|      100|     17.11|     214.7|         88|        9.66|      9.7|         4|       2.62|        2|\n",
      "|    0| 510|    1|         0|   119.3|       82|     20.28|   185.1|      111|     15.73|     157.0|         74|        7.07|     10.9|         4|       2.94|        2|\n",
      "|    0| 408|    1|         0|   242.5|       82|     41.23|   232.9|       97|     19.80|     154.0|         86|        6.93|      9.6|         7|       2.59|        0|\n",
      "|    1| 408|    0|        18|   222.1|       89|     37.76|   160.6|      109|     13.65|     218.8|        102|        9.85|     13.6|         2|       3.67|        0|\n",
      "+-----+----+-----+----------+--------+---------+----------+--------+---------+----------+----------+-----------+------------+---------+----------+-----------+---------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "sparkSession = spark \\\n",
    "        .builder \\\n",
    "        .getOrCreate()\n",
    "\n",
    "df = sparkSession.read \\\n",
    "        .format(\"com.ibm.idax.spark.idaxsource\") \\\n",
    "        .options(dbtable=\"SAMPLES.TRAINING\") \\\n",
    "        .load()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many records do we have? how many features? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 3333\n",
      "Number of features: 17"
     ]
    }
   ],
   "source": [
    "print('Number of records: '+str(df.count()))\n",
    "print('Number of features: '+str(len(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Split the data__\n",
    "\n",
    "We split the data into three datasets: for training, validation and testing. We use the proportions 80%, 10% and 10% so that we keep a relatively high number of examples for training but also have enough examples for the validation and test sets to be representative. Of course these proportions are subjective, you can change them if you want. We have also defined a seed so tht results can be reproduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_DF, val_DF, test_DF = df.randomSplit([0.80,0.10,0.10],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convert into Pandas DataFrame__\n",
    "\n",
    "In the following, we are going to use Python pandas and sklearn rather than Spark SQL and MLlib packages to prepare data and build models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>"
     ]
    }
   ],
   "source": [
    "(train, val, test) = (train_DF.toPandas(), val_DF.toPandas(), test_DF.toPandas())\n",
    "type(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore the data\n",
    "\n",
    "We only explore the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data schema__\n",
    "\n",
    "Let's see what kind of features we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CHURN: short (nullable = false)\n",
      " |-- AREA: integer (nullable = false)\n",
      " |-- VMAIL: short (nullable = false)\n",
      " |-- VMAIL_MSGS: integer (nullable = false)\n",
      " |-- DAY_MINS: decimal(5,1) (nullable = false)\n",
      " |-- DAY_CALLS: integer (nullable = false)\n",
      " |-- DAY_CHARGE: decimal(5,2) (nullable = false)\n",
      " |-- EVE_MINS: decimal(5,1) (nullable = false)\n",
      " |-- EVE_CALLS: integer (nullable = false)\n",
      " |-- EVE_CHARGE: decimal(5,2) (nullable = false)\n",
      " |-- NIGHT_MINS: decimal(5,1) (nullable = false)\n",
      " |-- NIGHT_CALLS: integer (nullable = false)\n",
      " |-- NIGHT_CHARGE: decimal(5,2) (nullable = false)\n",
      " |-- INTL_MINS: decimal(4,1) (nullable = false)\n",
      " |-- INTL_CALLS: integer (nullable = false)\n",
      " |-- INTL_CHARGE: decimal(4,2) (nullable = false)\n",
      " |-- SVC_CALLS: integer (nullable = false)"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns include:\n",
    "\n",
    "- Churn : whether the customer resigned his/her phone subscription, this is our target\n",
    "- Area : a geographic label, we might wonder which impact this feature has on churn\n",
    "- VMail: whether voice messages have been left\n",
    "- VMail_msgs: number of voice messages\n",
    "- day_mins, eve_mins, night_mins, intl_mins: time spend calling in the day/evening/night/abroad\n",
    "- day_calls, eve_calls, night_calls, intl_calls: number of calls in each category\n",
    "- day_charge, eve_charge, night_charge, intl_charge: price charged for each category\n",
    "- svc_calls: service calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Basic statistics__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0                    1                    2     3      4\n",
      "summary       count                 mean               stddev   min    max\n",
      "CHURN          2639  0.14437286851079956  0.35153402430151326     0      1\n",
      "AREA           2639   437.88291019325504   42.760111658777824   408    510\n",
      "VMAIL          2639   0.7154225085259568   0.4512984820521502     0      1\n",
      "VMAIL_MSGS     2639    8.291777188328913   13.771548440946265     0     51\n",
      "DAY_MINS       2639            179.37378    53.74770593138116   0.0  350.8\n",
      "DAY_CALLS      2639   100.39370973853732   19.769098649235545     0    163\n",
      "DAY_CHARGE     2639            30.494096     9.13705752690767  0.00  59.64\n",
      "EVE_MINS       2639            202.11849    50.53342222518002   0.0  354.2\n",
      "EVE_CALLS      2639   100.09814323607426    19.90713448426147     0    170\n",
      "EVE_CHARGE     2639            17.180288    4.295333768626179  0.00  30.11\n",
      "NIGHT_MINS     2639            200.49716    50.42337557025347  23.2  395.0\n",
      "NIGHT_CALLS    2639   100.09018567639258   19.666481221290642    36    175\n",
      "NIGHT_CHARGE   2639             9.022437    2.269126459115857  1.04  17.77\n",
      "INTL_MINS      2639             10.24869   2.7733851648926118   0.0   20.0\n",
      "INTL_CALLS     2639    4.476316786661614     2.46939337465794     0     20\n",
      "INTL_CHARGE    2639             2.767620   0.7487718667237856  0.00   5.40\n",
      "SVC_CALLS      2639   1.5532398635846911   1.3043738456928857     0      9"
     ]
    }
   ],
   "source": [
    "train_DF.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is no missing value in the dataset, which is great.\n",
    "\n",
    "We have a turnout of roughly 15%, so we must do better than a 85% accuracy (accuracy of a naive classifier which would label all examples as 0 id est customer remains a client).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments\n",
    "- most people who have voice mail messages are likely to end their contract, particularly if they receive between 20 and 40 voice messages;\n",
    "- distribution of number of calls and cumulated time among each category is very similar between classes churn=1 and churn=0. This might make our prediction task hard to perform;\n",
    "- service calls distributions however differ from class 0 to class 1; \n",
    "- area is not a discriminative feauture;\n",
    "- concentric round patterns tend to appear but the red zone is superimposed with the blue underlying zone, so this might also make prediction hard;\n",
    "- pricing is linear (simple linear function between time and price, no flat rate or minimum charge).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Add a new column__\n",
    "\n",
    "Here we create a new column `TOT_MINS` defined as the sum of all call durations. Since charge and duration are bound by a linear function, we will not used `DAY_MINS`, `EVE_MINS`, `INTL_MINS` and `NIGHT_MINS` in the model, just the corresponding `_CHARGE` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def TOT_MINS(dataframe):\n",
    "    dataframe[\"TOT_MINS\"]=dataframe['DAY_MINS']+dataframe['EVE_MINS']+dataframe['INTL_MINS']+dataframe['NIGHT_MINS']\n",
    "    return dataframe\n",
    "\n",
    "train, val, test = TOT_MINS(train), TOT_MINS(val), TOT_MINS(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extract the target vector__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_y=train[\"CHURN\"]\n",
    "val_y=val[\"CHURN\"]\n",
    "test_y=test[\"CHURN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Feature selection with Chi-Test__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Chi-test for k best features selection\n",
    "\n",
    "def select_col(X, y, k):\n",
    "    \"\"\"\n",
    "    X: only the features\n",
    "    y: target vector for X\n",
    "    k: number of best features to select\n",
    "    \"\"\"    \n",
    "    return SelectKBest(chi2, k=k).fit_transform(X,y)\n",
    "\n",
    "new_train=select_col(train.drop(\"CHURN\", axis=1), train_y, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0      1      2      3  4      5\n",
      "0  12  200.3  34.05  253.6  0  613.4\n",
      "1  13  193.1  32.83  111.6  1  544.2\n",
      "2  13  207.6  35.29  152.7  1  602.8\n",
      "3  14   80.2  13.63  219.0  1  430.4\n",
      "4  15  159.3  27.08  170.6  1  483.0"
     ]
    }
   ],
   "source": [
    "# ake a look at the new_train, we have to convert the array back to a pandas dataframe\n",
    "type(new_train)\n",
    "new_train=pd.DataFrame(new_train)\n",
    "new_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names have been dropped by SelectKBest. We write them here for you and explicitly define the datafames we will use in the next steps for more clarity. The following features have been selected : TOT_MINS, SVC_CALLS, VMAIL_MSGS, DAY_CHARGE, EVE_CHARGE, INTL_CALLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_x=train[[\"TOT_MINS\", \"SVC_CALLS\", \"VMAIL_MSGS\", \"DAY_CHARGE\", \"EVE_CHARGE\", \"INTL_CALLS\"]]\n",
    "\n",
    "val_x=val[[\"TOT_MINS\", \"SVC_CALLS\", \"VMAIL_MSGS\", \"DAY_CHARGE\", \"EVE_CHARGE\", \"INTL_CALLS\"]]\n",
    "\n",
    "test_x=test[[\"TOT_MINS\", \"SVC_CALLS\", \"VMAIL_MSGS\", \"DAY_CHARGE\", \"EVE_CHARGE\", \"INTL_CALLS\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__First try with default options__\n",
    "\n",
    "Let's define and build a logistic regression model on our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logReg = linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1, fit_intercept=True, \n",
    "                                        intercept_scaling=1, class_weight=None, random_state=None, \n",
    "                                        solver='liblinear', max_iter=100, multi_class='auto', verbose=0, \n",
    "                                        warm_start=False, n_jobs=None, l1_ratio=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = logReg.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Metrics__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set\n",
      "0.8582796513830997"
     ]
    }
   ],
   "source": [
    "#Accuracy\n",
    "print(\"Accuracy on training set\")\n",
    "model.score(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just above the accuracy of a naive classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tn, fp, fn, tp)=(2242, 16, 358, 23)\n",
      "Recall: 0.06036745406824147\n",
      "Precision: 0.5897435897435898\n",
      "f1-score: 0.10952380952380952"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "train_pred=model.predict(train_x)\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(train_y, train_pred).ravel()\n",
    "print(\"(tn, fp, fn, tp)=\"+str((tn, fp, fn, tp)))\n",
    "recall=tp/(tp+fn)\n",
    "precision=tp/(tp+fp)\n",
    "print(\"Recall: \"+str(recall))\n",
    "print(\"Precision: \"+str(precision))\n",
    "print(\"f1-score: \"+str(2*(precision*recall)/(precision+recall)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fine tune your hyperparameters on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8304597701149425"
     ]
    }
   ],
   "source": [
    "# With our default values : accuracy\n",
    "model.score(val_x, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285 4 55 4\n",
      "Recall: 0.06779661016949153\n",
      "Precision: 0.5\n",
      "f1-score: 0.11940298507462686"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "val_pred=model.predict(val_x)\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(val_y, val_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "recall=tp/(tp+fn)\n",
    "precision=tp/(tp+fp)\n",
    "print(\"Recall: \"+str(recall))\n",
    "print(\"Precision: \"+str(precision))\n",
    "print(\"f1-score: \"+str(2*(precision*recall)/(precision+recall)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Grid Search__\n",
    "\n",
    "We want to maximize recall id est the proportion of positive cases detected as such over the total number of real positive cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def grid_search():\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    DF : Spark Dataframe, prepared with features and label columns\n",
    "    \n",
    "    Output:\n",
    "    best: dictionary with best hyperparameters and accuracies\n",
    "    \"\"\"\n",
    "    \n",
    "    # Hyperparameter ranges\n",
    "    tol_range=[5e-3, 1e-4, 1e-5]\n",
    "    C_range=[0.02, 0.05, 0.1, 0.5, 1]\n",
    "    intercept_scaling_range=[50, 20, 10, 2, 1]\n",
    "    \n",
    "    best={\"tol\":0, \"C\": 0, \"intercept_scaling\": 0, \"training_accuracy\": 0, \"validation_accuracy\": 0, \"recall\":0}\n",
    "          \n",
    "    for tol in tol_range:\n",
    "        for C in C_range:\n",
    "            for i in intercept_scaling_range:\n",
    "            \n",
    "                print(\"tol = \"+str(tol)+\" , C = \"+ str(C), \"intercept_scaling = \"+str(i))\n",
    "\n",
    "                # Define your model\n",
    "                logReg = linear_model.LogisticRegression(penalty='l2', dual=False, tol=tol, C=C, fit_intercept=True, \n",
    "                                            intercept_scaling=i, class_weight=None, random_state=None, \n",
    "                                            solver='liblinear', max_iter=100, multi_class='auto', verbose=0, \n",
    "                                            warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "\n",
    "                # Fit the model on training set\n",
    "                model = logReg.fit(train_x, train_y)\n",
    "\n",
    "                # Make predictions on validation set\n",
    "                val_pred = model.predict(val_x)\n",
    "\n",
    "                # Compute the accuracy on both datasets\n",
    "                train_acc = model.score(train_x, train_y)\n",
    "                val_acc = model.score(val_x, val_y)\n",
    "                print(\"Training Accuracy = %g\" %  train_acc)\n",
    "                print(\"Validation Accuracy = %g\" % val_acc)\n",
    "\n",
    "                # Compute recall\n",
    "                tn, fp, fn, tp = metrics.confusion_matrix(val_y, val_pred).ravel()\n",
    "                recall = tp/(tp+fn)\n",
    "                print(\"Validation recall: \"+str(recall))\n",
    "                print(\"\")\n",
    "\n",
    "                # Find the best model\n",
    "                if recall >= best[\"recall\"]:\n",
    "                    best[\"tol\"] = tol\n",
    "                    best[\"C\"] = C\n",
    "                    best[\"intercept_scaling\"] = i\n",
    "                    best[\"training_accuracy\"] = train_acc\n",
    "                    best[\"validation_accuracy\"] = val_acc\n",
    "                    best[\"recall\"]=recall\n",
    "                \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol = 0.005 , C = 0.02 intercept_scaling = 50\n",
      "Training Accuracy = 0.861311\n",
      "Validation Accuracy = 0.836207\n",
      "Validation recall: 0.03389830508474576\n",
      "\n",
      "tol = 0.005 , C = 0.02 intercept_scaling = 20\n",
      "Training Accuracy = 0.861311\n",
      "Validation Accuracy = 0.833333\n",
      "Validation recall: 0.01694915254237288\n",
      "\n",
      "tol = 0.005 , C = 0.02 intercept_scaling = 10\n",
      "Training Accuracy = 0.856764\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.0\n",
      "\n",
      "tol = 0.005 , C = 0.02 intercept_scaling = 2\n",
      "Training Accuracy = 0.856764\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.01694915254237288\n",
      "\n",
      "tol = 0.005 , C = 0.02 intercept_scaling = 1\n",
      "Training Accuracy = 0.855248\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.01694915254237288\n",
      "\n",
      "tol = 0.005 , C = 0.05 intercept_scaling = 50\n",
      "Training Accuracy = 0.86169\n",
      "Validation Accuracy = 0.836207\n",
      "Validation recall: 0.03389830508474576\n",
      "\n",
      "tol = 0.005 , C = 0.05 intercept_scaling = 20\n",
      "Training Accuracy = 0.86169\n",
      "Validation Accuracy = 0.836207\n",
      "Validation recall: 0.03389830508474576\n",
      "\n",
      "tol = 0.005 , C = 0.05 intercept_scaling = 10\n",
      "Training Accuracy = 0.856764\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.0\n",
      "\n",
      "tol = 0.005 , C = 0.05 intercept_scaling = 2\n",
      "Training Accuracy = 0.855627\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.01694915254237288\n",
      "\n",
      "tol = 0.005 , C = 0.05 intercept_scaling = 1\n",
      "Training Accuracy = 0.855248\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.01694915254237288\n",
      "\n",
      "tol = 0.005 , C = 0.1 intercept_scaling = 50\n",
      "Training Accuracy = 0.86169\n",
      "Validation Accuracy = 0.836207\n",
      "Validation recall: 0.03389830508474576\n",
      "\n",
      "tol = 0.005 , C = 0.1 intercept_scaling = 20\n",
      "Training Accuracy = 0.86169\n",
      "Validation Accuracy = 0.836207\n",
      "Validation recall: 0.03389830508474576\n",
      "\n",
      "tol = 0.005 , C = 0.1 intercept_scaling = 10\n",
      "Training Accuracy = 0.856764\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.0\n",
      "\n",
      "tol = 0.005 , C = 0.1 intercept_scaling = 2\n",
      "Training Accuracy = 0.856385\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.01694915254237288\n",
      "\n",
      "tol = 0.005 , C = 0.1 intercept_scaling = 1\n",
      "Training Accuracy = 0.855248\n",
      "Validation Accuracy = 0.827586\n",
      "Validation recall: 0.01694915254237288\n",
      "\n",
      "tol = 0.005 , C = 0.5 intercept_scaling = 50\n",
      "Training Accuracy = 0.863585\n",
      "Validation Accuracy = 0.836207\n",
      "Validation recall: 0.03389830508474576\n",
      "\n",
      "tol = 0.005 , C = 0.5 intercept_scaling = 20\n",
      "Training Accuracy = 0.862069\n",
      "Validation Accuracy = 0.836207\n",
      "Validation recall: 0.03389830508474576\n",
      "\n",
      "tol = 0.005 , C = 0.5 intercept_scaling = 10\n",
      "Training Accuracy = 0.856764\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.0\n",
      "\n",
      "tol = 0.005 , C = 0.5 intercept_scaling = 2\n",
      "Training Accuracy = 0.854869\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.0\n",
      "\n",
      "tol = 0.005 , C = 0.5 intercept_scaling = 1\n",
      "Training Accuracy = 0.855248\n",
      "Validation Accuracy = 0.827586\n",
      "Validation recall: 0.01694915254237288\n",
      "\n",
      "tol = 0.005 , C = 1 intercept_scaling = 50\n",
      "Training Accuracy = 0.863585\n",
      "Validation Accuracy = 0.836207\n",
      "Validation recall: 0.03389830508474576\n",
      "\n",
      "tol = 0.005 , C = 1 intercept_scaling = 20\n",
      "Training Accuracy = 0.862069\n",
      "Validation Accuracy = 0.836207\n",
      "Validation recall: 0.03389830508474576\n",
      "\n",
      "tol = 0.005 , C = 1 intercept_scaling = 10\n",
      "Training Accuracy = 0.856764\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.0\n",
      "\n",
      "tol = 0.005 , C = 1 intercept_scaling = 2\n",
      "Training Accuracy = 0.854869\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.0\n",
      "\n",
      "tol = 0.005 , C = 1 intercept_scaling = 1\n",
      "Training Accuracy = 0.855248\n",
      "Validation Accuracy = 0.827586\n",
      "Validation recall: 0.01694915254237288\n",
      "\n",
      "tol = 0.0001 , C = 0.02 intercept_scaling = 50\n",
      "Training Accuracy = 0.86169\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.06779661016949153\n",
      "\n",
      "tol = 0.0001 , C = 0.02 intercept_scaling = 20\n",
      "Training Accuracy = 0.86169\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.06779661016949153\n",
      "\n",
      "tol = 0.0001 , C = 0.02 intercept_scaling = 10\n",
      "Training Accuracy = 0.860932\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.06779661016949153\n",
      "\n",
      "tol = 0.0001 , C = 0.02 intercept_scaling = 2\n",
      "Training Accuracy = 0.857901\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.01694915254237288\n",
      "\n",
      "tol = 0.0001 , C = 0.02 intercept_scaling = 1\n",
      "Training Accuracy = 0.857522\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.01694915254237288\n",
      "\n",
      "tol = 0.0001 , C = 0.05 intercept_scaling = 50\n",
      "Training Accuracy = 0.862448\n",
      "Validation Accuracy = 0.833333\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 0.0001 , C = 0.05 intercept_scaling = 20\n",
      "Training Accuracy = 0.862069\n",
      "Validation Accuracy = 0.833333\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 0.0001 , C = 0.05 intercept_scaling = 10\n",
      "Training Accuracy = 0.862069\n",
      "Validation Accuracy = 0.833333\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 0.0001 , C = 0.05 intercept_scaling = 2\n",
      "Training Accuracy = 0.857901\n",
      "Validation Accuracy = 0.827586\n",
      "Validation recall: 0.01694915254237288\n",
      "\n",
      "tol = 0.0001 , C = 0.05 intercept_scaling = 1\n",
      "Training Accuracy = 0.856764\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.01694915254237288\n",
      "\n",
      "tol = 0.0001 , C = 0.1 intercept_scaling = 50\n",
      "Training Accuracy = 0.862069\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 0.0001 , C = 0.1 intercept_scaling = 20\n",
      "Training Accuracy = 0.862069\n",
      "Validation Accuracy = 0.833333\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 0.0001 , C = 0.1 intercept_scaling = 10\n",
      "Training Accuracy = 0.86169\n",
      "Validation Accuracy = 0.833333\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 0.0001 , C = 0.1 intercept_scaling = 2\n",
      "Training Accuracy = 0.858659\n",
      "Validation Accuracy = 0.827586\n",
      "Validation recall: 0.01694915254237288\n",
      "\n",
      "tol = 0.0001 , C = 0.1 intercept_scaling = 1\n",
      "Training Accuracy = 0.856764\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.01694915254237288\n",
      "\n",
      "tol = 0.0001 , C = 0.5 intercept_scaling = 50\n",
      "Training Accuracy = 0.860932\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 0.0001 , C = 0.5 intercept_scaling = 20\n",
      "Training Accuracy = 0.860932\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 0.0001 , C = 0.5 intercept_scaling = 10\n",
      "Training Accuracy = 0.860932\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 0.0001 , C = 0.5 intercept_scaling = 2\n",
      "Training Accuracy = 0.859416\n",
      "Validation Accuracy = 0.833333\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 0.0001 , C = 0.5 intercept_scaling = 1\n",
      "Training Accuracy = 0.85828\n",
      "Validation Accuracy = 0.824713\n",
      "Validation recall: 0.01694915254237288\n",
      "\n",
      "tol = 0.0001 , C = 1 intercept_scaling = 50\n",
      "Training Accuracy = 0.860932\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 0.0001 , C = 1 intercept_scaling = 20\n",
      "Training Accuracy = 0.860932\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 0.0001 , C = 1 intercept_scaling = 10\n",
      "Training Accuracy = 0.860932\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 0.0001 , C = 1 intercept_scaling = 2\n",
      "Training Accuracy = 0.860553\n",
      "Validation Accuracy = 0.833333\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 0.0001 , C = 1 intercept_scaling = 1\n",
      "Training Accuracy = 0.85828\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.06779661016949153\n",
      "\n",
      "tol = 1e-05 , C = 0.02 intercept_scaling = 50\n",
      "Training Accuracy = 0.862069\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.06779661016949153\n",
      "\n",
      "tol = 1e-05 , C = 0.02 intercept_scaling = 20\n",
      "Training Accuracy = 0.86169\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.06779661016949153\n",
      "\n",
      "tol = 1e-05 , C = 0.02 intercept_scaling = 10\n",
      "Training Accuracy = 0.860932\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.06779661016949153\n",
      "\n",
      "tol = 1e-05 , C = 0.02 intercept_scaling = 2\n",
      "Training Accuracy = 0.857901\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.01694915254237288\n",
      "\n",
      "tol = 1e-05 , C = 0.02 intercept_scaling = 1\n",
      "Training Accuracy = 0.857522\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.01694915254237288\n",
      "\n",
      "tol = 1e-05 , C = 0.05 intercept_scaling = 50\n",
      "Training Accuracy = 0.862448\n",
      "Validation Accuracy = 0.833333\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 1e-05 , C = 0.05 intercept_scaling = 20\n",
      "Training Accuracy = 0.862069\n",
      "Validation Accuracy = 0.833333\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 1e-05 , C = 0.05 intercept_scaling = 10\n",
      "Training Accuracy = 0.86169\n",
      "Validation Accuracy = 0.833333\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 1e-05 , C = 0.05 intercept_scaling = 2\n",
      "Training Accuracy = 0.857901\n",
      "Validation Accuracy = 0.827586\n",
      "Validation recall: 0.01694915254237288\n",
      "\n",
      "tol = 1e-05 , C = 0.05 intercept_scaling = 1\n",
      "Training Accuracy = 0.856764\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.01694915254237288\n",
      "\n",
      "tol = 1e-05 , C = 0.1 intercept_scaling = 50\n",
      "Training Accuracy = 0.86169\n",
      "Validation Accuracy = 0.833333\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 1e-05 , C = 0.1 intercept_scaling = 20\n",
      "Training Accuracy = 0.862069\n",
      "Validation Accuracy = 0.833333\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 1e-05 , C = 0.1 intercept_scaling = 10\n",
      "Training Accuracy = 0.86169\n",
      "Validation Accuracy = 0.833333\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 1e-05 , C = 0.1 intercept_scaling = 2\n",
      "Training Accuracy = 0.858659\n",
      "Validation Accuracy = 0.827586\n",
      "Validation recall: 0.01694915254237288\n",
      "\n",
      "tol = 1e-05 , C = 0.1 intercept_scaling = 1\n",
      "Training Accuracy = 0.856764\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.01694915254237288\n",
      "\n",
      "tol = 1e-05 , C = 0.5 intercept_scaling = 50\n",
      "Training Accuracy = 0.860932\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 1e-05 , C = 0.5 intercept_scaling = 20\n",
      "Training Accuracy = 0.860932\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 1e-05 , C = 0.5 intercept_scaling = 10\n",
      "Training Accuracy = 0.860932\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 1e-05 , C = 0.5 intercept_scaling = 2\n",
      "Training Accuracy = 0.859795\n",
      "Validation Accuracy = 0.833333\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 1e-05 , C = 0.5 intercept_scaling = 1\n",
      "Training Accuracy = 0.858659\n",
      "Validation Accuracy = 0.824713\n",
      "Validation recall: 0.01694915254237288\n",
      "\n",
      "tol = 1e-05 , C = 1 intercept_scaling = 50\n",
      "Training Accuracy = 0.860932\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 1e-05 , C = 1 intercept_scaling = 20\n",
      "Training Accuracy = 0.860932\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 1e-05 , C = 1 intercept_scaling = 10\n",
      "Training Accuracy = 0.860932\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 1e-05 , C = 1 intercept_scaling = 2\n",
      "Training Accuracy = 0.860932\n",
      "Validation Accuracy = 0.833333\n",
      "Validation recall: 0.0847457627118644\n",
      "\n",
      "tol = 1e-05 , C = 1 intercept_scaling = 1\n",
      "Training Accuracy = 0.859416\n",
      "Validation Accuracy = 0.83046\n",
      "Validation recall: 0.06779661016949153\n",
      "\n",
      "{'tol': 1e-05, 'C': 1, 'intercept_scaling': 2, 'training_accuracy': 0.8609321712769988, 'validation_accuracy': 0.8333333333333334, 'recall': 0.0847457627118644}"
     ]
    }
   ],
   "source": [
    "best=grid_search()\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Assess your model performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8901734104046243\n",
      "(tn, fp, fn, tp)=(302, 1, 37, 6)\n",
      "Recall: 0.13953488372093023"
     ]
    }
   ],
   "source": [
    "# Define the model with the hyperparameters you have finally chosen\n",
    "logReg = linear_model.LogisticRegression(penalty='l2', dual=False, tol=best[\"tol\"], C=best[\"C\"], fit_intercept=True, \n",
    "                                            intercept_scaling=best[\"intercept_scaling\"], class_weight=None, random_state=None, \n",
    "                                            solver='liblinear', max_iter=100, multi_class='auto', verbose=0, \n",
    "                                            warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "# Fit it to your training set\n",
    "model=logReg.fit(train_x, train_y)\n",
    "\n",
    "\n",
    "# Make prediction\n",
    "test_pred=model.predict(test_x)\n",
    "\n",
    "# Compute test accuracy\n",
    "print(\"Accuracy: \"+str(model.score(test_x, test_y)))\n",
    "\n",
    "# Compute test recall\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(test_y, test_pred).ravel()\n",
    "print(\"(tn, fp, fn, tp)=\"+str((tn, fp, fn, tp)))\n",
    "print(\"Recall: \"+str(tp/(tp+fn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison : with more features (\"TOT_MINS\", \"SVC_CALLS\", \"VMAIL\", \"VMAIL_MSGS\", \"DAY_CHARGE\", \"EVE_CHARGE\", \"NIGHT_CHARGE\", \"INTL_CALLS\", \"INTL_CHARGE\") we obtained:\n",
    "\n",
    "* 'tol': 1e-05, \n",
    "* 'C': 1, \n",
    "* 'intercept_scaling': 10, \n",
    "* 'training_accuracy': 0.8582796513830997, \n",
    "* 'validation_accuracy': 0.8390804597701149, \n",
    "* 'recall': 0.1016949152542373\n",
    "\n",
    "And on the test set:\n",
    "* Accuracy: 0.884393063583815\n",
    "* (tn, fp, fn, tp)=(300, 3, 37, 6)\n",
    "* Recall: 0.13953488372093023\n",
    "\n",
    "So in our case we have the same recall but a slightly better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Comment on test results__\n",
    "\n",
    "* 0.89 > 0.85 so we have done better than a naive classifier for accuracy.\n",
    "* Thanks to hyperparameter tuning, recall was improved by 7% (6.8% -> 14%).\n",
    "* Recall is still not high but our data didn't allow us to do much better.\n",
    "* So we have a satisfying result granted that we developed a very basic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save your model for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Take a look at parameters and hyperparameters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 2, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'liblinear', 'tol': 1e-05, 'verbose': 0, 'warm_start': False}"
     ]
    }
   ],
   "source": [
    "# hyperparameters of the model\n",
    "hp=model.get_params()\n",
    "print(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# how to transfer hyperparameters to another class\n",
    "logReg2=linear_model.LogisticRegression().set_params(**hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters for chosen features\n",
      "[[ 0.00212649  0.4402351  -0.022415    0.05987647  0.0391706  -0.08033731]]\n",
      "Intercept\n",
      "[-5.98505789]"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "print(\"Parameters for chosen features\")\n",
    "print(model.coef_)\n",
    "print(\"Intercept\")\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Save your model with Joblib__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(model, '/tmp/saved_model.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = load('/tmp/saved_model.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it : you should get the same result as in the previous cell\n",
    "test_pred2 = saved_model.predict(test_x)\n",
    "saved_model.score(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you've learned\n",
    "\n",
    "Congratulations!\n",
    "\n",
    "In this notebook you have seen how to:\n",
    "* fetch sample data from Db2 using PySpark\n",
    "* use sklearn to build a logistic regression \n",
    "* deploy a classic ML workflow from data exploration to model assessment\n",
    "* save your model for future use - in a Python UDF function for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____ \n",
    "## Authors\n",
    "\n",
    "Eva Feillet - ML intern, IBM Cloud and Cognitive Software, Böblingen, Germany"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Licence__\n",
    " --> to add"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
