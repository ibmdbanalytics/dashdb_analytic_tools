{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction + Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative #3 tried: 12 Features, MinMax scaling, no ClusterChurn, Cross validation with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Imports__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>155</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Useful imports - PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load the data__\n",
    "\n",
    "Let's use a table which has been pre-populated in Db2 local. It is called SAMPLES.TRAINING. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+----------+--------+---------+----------+--------+---------+----------+----------+-----------+------------+---------+----------+-----------+---------+\n",
      "|CHURN|AREA|VMAIL|VMAIL_MSGS|DAY_MINS|DAY_CALLS|DAY_CHARGE|EVE_MINS|EVE_CALLS|EVE_CHARGE|NIGHT_MINS|NIGHT_CALLS|NIGHT_CHARGE|INTL_MINS|INTL_CALLS|INTL_CHARGE|SVC_CALLS|\n",
      "+-----+----+-----+----------+--------+---------+----------+--------+---------+----------+----------+-----------+------------+---------+----------+-----------+---------+\n",
      "|    0| 415|    1|         0|   246.5|      108|     41.91|   216.3|       89|     18.39|     179.6|         99|        8.08|     12.7|         3|       3.43|        2|\n",
      "|    1| 408|    1|         0|   298.1|      112|     50.68|   201.3|      100|     17.11|     214.7|         88|        9.66|      9.7|         4|       2.62|        2|\n",
      "|    0| 510|    1|         0|   119.3|       82|     20.28|   185.1|      111|     15.73|     157.0|         74|        7.07|     10.9|         4|       2.94|        2|\n",
      "|    0| 408|    1|         0|   242.5|       82|     41.23|   232.9|       97|     19.80|     154.0|         86|        6.93|      9.6|         7|       2.59|        0|\n",
      "|    1| 408|    0|        18|   222.1|       89|     37.76|   160.6|      109|     13.65|     218.8|        102|        9.85|     13.6|         2|       3.67|        0|\n",
      "+-----+----+-----+----------+--------+---------+----------+--------+---------+----------+----------+-----------+------------+---------+----------+-----------+---------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "sparkSession = SparkSession \\\n",
    "        .builder \\\n",
    "        .getOrCreate()\n",
    "\n",
    "df = sparkSession.read \\\n",
    "        .format(\"com.ibm.idax.spark.idaxsource\") \\\n",
    "        .options(dbtable=\"SAMPLES.TRAINING\") \\\n",
    "        .load()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many records do we have? How many features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 3333\n",
      "Number of features: 17"
     ]
    }
   ],
   "source": [
    "print('Number of records: '+str(df.count()))\n",
    "print('Number of features: '+str(len(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Split the data__\n",
    "\n",
    "We split the data into three distinct sets: for training, validation and testing. We use the proportions 70%, 15% and 15% so that we keep a relatively high number of examples for training. Of course these proportions are subjective, you can change them if you want. We have defined a seed so that results can be reproduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cross_train, test = df.withColumnRenamed(\"CHURN\", \"label\").randomSplit([0.85,0.15],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Evaluation criterion : Area under Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define the pipeline__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "# Prepare training documents, which are labeled.\n",
    "# input : cross_train\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: assembler_12, MinMaxScaler, and lr.\n",
    "\n",
    "assembler_12 = VectorAssembler(\n",
    "    inputCols=[\"SVC_CALLS\", \"DAY_MINS\", \"DAY_CHARGE\", \"VMAIL_MSGS\", \"VMAIL\", \n",
    "               \"INTL_CALLS\", \"INTL_CHARGE\", \"INTL_MINS\", \"EVE_CHARGE\", \"EVE_MINS\",\n",
    "               \"NIGHT_MINS\", \"NIGHT_CHARGE\"],\n",
    "    outputCol=\"features12\")\n",
    "\n",
    "scaler = MinMaxScaler(inputCol = \"features12\", outputCol = \"features\")\n",
    "# scalerModel = scaler.fit(train_12) ISSUE ?\n",
    "\n",
    "# scaled_train = scalerModel.transform(train_12)\n",
    "\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "pipeline = Pipeline(stages=[assembler_12, scaler, lr])\n",
    "\n",
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.001, 0.002, 0.005, 0.01]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.001, 0.002, 0.005, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator().setMetricName(\"areaUnderPR\"),\n",
    "                          numFolds=8)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(cross_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Evaluation on test set__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "# need for assembling and scaling ?\n",
    "pred_test = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'areaUnderROC'"
     ]
    }
   ],
   "source": [
    "#by default\n",
    "BinaryClassificationEvaluator().getMetricName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC: 0.759\n",
      "Area under PR curve: 0.439"
     ]
    }
   ],
   "source": [
    "binEval_AUROC = BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\").setRawPredictionCol(\"rawPrediction\").setLabelCol(\"label\")\n",
    "print(\"Area under ROC: %.3f\" % binEval_AUROC.evaluate(pred_test))\n",
    "\n",
    "binEval_AUPRC = BinaryClassificationEvaluator().setMetricName(\"areaUnderPR\").setRawPredictionCol(\"rawPrediction\").setLabelCol(\"label\")\n",
    "print(\"Area under PR curve: %.3f\" % binEval_AUPRC.evaluate(pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.878\n",
      "f1 = 0.835\n",
      "weightedPrecision = 0.893"
     ]
    }
   ],
   "source": [
    "# Metrics (1/2)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(pred_test)\n",
    "print(\"Accuracy = %.3f\" % accuracy)\n",
    "\n",
    "evaluatorf1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluatorf1.evaluate(pred_test)\n",
    "print(\"f1 = %.3f\" % f1)\n",
    " \n",
    "evaluatorwp = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "wp = evaluatorwp.evaluate(pred_test)\n",
    "print(\"weightedPrecision = %.3f\" % wp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Confusion matrix \n",
    "\n",
    "<img src=\"confusionMatrix.png\" width=800/>\n",
    "\n",
    "Sensitivity = TP / (TP + FN), also called Recall\n",
    "\n",
    "Specificity = TN / (TN + FP)\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "Negative Predictive Value = TN / (TN + FN)\n",
    "\n",
    "F1-score: 2 x Precision x Recall / (Precision + Recall)\n",
    "\n",
    "Recall = 1 - FNR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Metrics (2/2)\n",
    "\n",
    "def confusion_matrix(pred_DF):\n",
    "    \"\"\"\n",
    "    Input : \n",
    "    pred_DF : Saprk DataFrame obtained after a model.transform() transformation\n",
    "    Output :\n",
    "    (tn, fp, fn, tp) tuple of integers\n",
    "    \"\"\"\n",
    "    \n",
    "    # as pandas DF\n",
    "    label = pred_DF.toPandas()[\"label\"]\n",
    "    prediction = pred_DF.toPandas()[\"prediction\"]\n",
    "    \n",
    "    # true = tp + tn\n",
    "    true = sum(label)\n",
    "\n",
    "    # tp : sum(if pred = label = 1) or count(pred+label=2)\n",
    "    tp = sum(label+prediction==2)\n",
    "    \n",
    "    # fn = total number of positive - predicted positives which are rightly predicted\n",
    "    fn = true - tp\n",
    "\n",
    "    # tn : sum(if pred=label=0) or count(pred+label=0)\n",
    "    tn = sum(label+prediction==0)\n",
    "\n",
    "    # fp = total number of negative - tn\n",
    "    fp = len(label) - true - tn\n",
    "    \n",
    "    return (tn, fp, fn, tp)\n",
    "\n",
    "\n",
    "def FNR(fn, tp):\n",
    "    return fn/(fn+tp)\n",
    "\n",
    "def recall(tp, fn):\n",
    "    # also called TPR or sensitivity\n",
    "    return tp/(tp+fn)\n",
    "\n",
    "def precision(tp, fp):\n",
    "    return tp/(tp+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "[['tp: 9' 'fn: 63']\n",
      " ['fp: 0' 'tn: 445']]\n",
      "FNR: 0.875\n",
      "Recall: 0.125\n",
      "Precision: 1.000"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "(tn, fp, fn, tp)=confusion_matrix(pred_test)\n",
    "# Confusion matrix in format [[tp, fn], [fp, tn]]\n",
    "print(\"Confusion matrix: \")\n",
    "print(np.array([[(\"tp: \"+str(tp)), (\"fn: \"+str(fn))], [(\"fp: \"+str(fp)), (\"tn: \"+str(tn))]]))\n",
    "\n",
    "# Metrics\n",
    "print(\"FNR: %.3f\" % FNR(fn, tp))\n",
    "print(\"Recall: %.3f\" % recall(tp, fn))\n",
    "print(\"Precision: %.3f\" % precision(tp, fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Evaluation criterion: Accuracy\n",
    "\n",
    "recall :  only weightedRecalll can be set as metric name ! is the same as accuracy for a binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define the pipeline__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### With MulticlassClassificationEvaluator as evaluator\n",
    "\n",
    "# Prepare training documents, which are labeled.\n",
    "# input : cross_train\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: assembler_12, MinMaxScaler, and lr.\n",
    "\n",
    "assembler_12 = VectorAssembler(\n",
    "    inputCols=[\"SVC_CALLS\", \"DAY_MINS\", \"DAY_CHARGE\", \"VMAIL_MSGS\", \"VMAIL\", \n",
    "               \"INTL_CALLS\", \"INTL_CHARGE\", \"INTL_MINS\", \"EVE_CHARGE\", \"EVE_MINS\",\n",
    "               \"NIGHT_MINS\", \"NIGHT_CHARGE\"],\n",
    "    outputCol=\"features12\")\n",
    "\n",
    "scaler = MinMaxScaler(inputCol = \"features12\", outputCol = \"features\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler_12, scaler, lr])\n",
    "\n",
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.001]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.001, 0.002]) \\\n",
    "    .build()\n",
    "\n",
    "# range  : [0.001, 0.002, 0.005, 0.01]\n",
    "# Evaluator\n",
    "\n",
    "multiEval = MulticlassClassificationEvaluator().setMetricName(\"f1\")\n",
    "\n",
    "crossvalR = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=multiEval,\n",
    "                          numFolds=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModelR = crossvalR.fit(cross_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Evaluation on test set__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "pred_testR = cvModelR.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.876\n",
      "f1 = 0.836\n",
      "weightedPrecision = 0.871"
     ]
    }
   ],
   "source": [
    "# Metrics (1/2)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(pred_testR)\n",
    "print(\"Accuracy = %.3f\" % accuracy)\n",
    "\n",
    "evaluatorf1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluatorf1.evaluate(pred_testR)\n",
    "print(\"f1 = %.3f\" % f1)\n",
    " \n",
    "evaluatorwp = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "wp = evaluatorwp.evaluate(pred_testR)\n",
    "print(\"weightedPrecision = %.3f\" % wp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "[['tp: 10' 'fn: 62']\n",
      " ['fp: 2' 'tn: 443']]\n",
      "FNR: 0.861\n",
      "Recall: 0.139\n",
      "Precision: 0.833"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "(tn, fp, fn, tp)=confusion_matrix(pred_testR)\n",
    "# Confusion matrix in format [[tp, fn], [fp, tn]]\n",
    "print(\"Confusion matrix: \")\n",
    "print(np.array([[(\"tp: \"+str(tp)), (\"fn: \"+str(fn))], [(\"fp: \"+str(fp)), (\"tn: \"+str(tn))]]))\n",
    "\n",
    "# Metrics\n",
    "print(\"FNR: %.3f\" % FNR(fn, tp))\n",
    "print(\"Recall: %.3f\" % recall(tp, fn))\n",
    "print(\"Precision: %.3f\" % precision(tp, fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
