{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction with IBM Db2 Warehouse using PySpark\n",
    "\n",
    "# Part 3 : Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Introduction__\n",
    "\n",
    "This notebook presents a churn prediction use case using anonymized customer data from a phone operator. It uses IBM Db2 Warehouse and runs on a PySpark kernel. It is the third part of a series on this use case. It is focused on deployment: in the previous notebook, we had saved our models. Let's reuse them on some fresh data!\n",
    "\n",
    "__Use case__\n",
    "\n",
    "Our goal is to accurately predict whether a customer is going to end his/her contract (labeled as positive,1). We prefer to send a commercial email to someone who intends to keep her contract but is labeled as willing to end it (false positive) rather than to overlook the opportunity of preventing a customer from ending her contract (false negative). We also care to accurately target customers with engagement campaigns : not overwhelming customers with commercials and not losing money by proposing special offers to too many people (precision and accuracy). Our optimization objective thus consisted in maximizing recall id est minimizing the false negative rate. We also looked at a couple of other indicators such as accuracy and area under curve.\n",
    "\n",
    "__Previously__\n",
    "\n",
    "In the first notebook, we used PySpark for data exploration and visualization. We created, scaled and selected features. In the second notebook, we built and tested several models. We selected the model with the highest recall on the test set. \n",
    "\n",
    "__Contents__\n",
    "1. Get ready\n",
    "2. Load fresh data\n",
    "3. Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Imports__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>183</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Basics\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Classification models\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.ml.classification import GBTClassificationModel\n",
    "from pyspark.ml.clustering import KMeansModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Open the models__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# note : please modify the path if you did not save the previous notebook in the same folder as this one\n",
    "lrModel = LogisticRegressionModel.load(\"/tmp/myLogRegModel\")\n",
    "rfModel = RandomForestClassificationModel.load(\"/tmp/myRFModel\")\n",
    "gbtModel = GBTClassificationModel.load(\"/tmp/myGBTModel\")\n",
    "clusterModel = KMeansModel.load(\"/tmp/myClusterModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|ClusterID|          avg(CHURN)|\n",
      "+---------+--------------------+\n",
      "|        0| 0.14545454545454545|\n",
      "|       13|  0.5445544554455446|\n",
      "|        4| 0.10365853658536585|\n",
      "|        8| 0.11278195488721804|\n",
      "|        6| 0.09815950920245399|\n",
      "|       11| 0.10052910052910052|\n",
      "|        1|0.049079754601226995|\n",
      "|        3| 0.06882591093117409|\n",
      "|       10| 0.07482993197278912|\n",
      "|        7| 0.07741935483870968|\n",
      "|        2| 0.13934426229508196|\n",
      "|       12| 0.14150943396226415|\n",
      "|        9|  0.5666666666666667|\n",
      "|       14| 0.07913669064748201|\n",
      "|        5|  0.0891089108910891|\n",
      "+---------+--------------------+"
     ]
    }
   ],
   "source": [
    "# open the CHURN_PROPORTION table you had previously saved\n",
    "churn_proportion = spark.read \\\n",
    "        .format(\"com.ibm.idax.spark.idaxsource\") \\\n",
    "        .options(dbtable=\"CHURN_PROPORTION\") \\\n",
    "        .load()\n",
    "\n",
    "churn_proportion.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load fresh data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Open the data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+--------+---------+----------+--------+---------+----------+----------+-----------+------------+---------+----------+-----------+---------+\n",
      "|AREA|VMAIL|VMAIL_MSGS|DAY_MINS|DAY_CALLS|DAY_CHARGE|EVE_MINS|EVE_CALLS|EVE_CHARGE|NIGHT_MINS|NIGHT_CALLS|NIGHT_CHARGE|INTL_MINS|INTL_CALLS|INTL_CHARGE|SVC_CALLS|\n",
      "+----+-----+----------+--------+---------+----------+--------+---------+----------+----------+-----------+------------+---------+----------+-----------+---------+\n",
      "| 415|    0|        25|   265.1|      110|     45.07|  197.40|       99|     16.78|     244.7|         91|       11.01|     10.0|         3|       2.70|        1|\n",
      "+----+-----+----------+--------+---------+----------+--------+---------+----------+----------+-----------+------------+---------+----------+-----------+---------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "# a table has been prepopulated in Db2 with a sample of unlabeled customer data : SAMPLES.EVAL\n",
    "sparkSession = spark \\\n",
    "        .builder \\\n",
    "        .getOrCreate()\n",
    "\n",
    "df = spark.read \\\n",
    "        .format(\"com.ibm.idax.spark.idaxsource\") \\\n",
    "        .options(dbtable=\"SAMPLES.EVAL\") \\\n",
    "        .load()\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows 4"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows \"+str(df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 4 new examples. They are not labeled. It's up to us to make predictions!\n",
    "\n",
    "But first, let's transform our data into a suitable format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Prepare the data__\n",
    "\n",
    "* Add ClusterChurn feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assembler for kmeans\n",
    "assembler_12 = VectorAssembler(\n",
    "    inputCols=[\"SVC_CALLS\", \"DAY_MINS\", \"DAY_CHARGE\", \"VMAIL_MSGS\", \"VMAIL\", \n",
    "               \"INTL_CALLS\", \"INTL_CHARGE\", \"INTL_MINS\", \"EVE_CHARGE\", \"EVE_MINS\",\n",
    "               \"NIGHT_MINS\", \"NIGHT_CHARGE\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "# Join tables on ClusterID to add a new column\n",
    "def preparation(DF):\n",
    "    \n",
    "    # assemble \n",
    "    DF_12 = assembler_12.transform(DF)\n",
    "    \n",
    "    # Assign each point of the training set to its cluster\n",
    "    DF_prediction = clusterModel.transform(DF_12)\n",
    "    \n",
    "    # Join DF with table churn_proportion on ClusterID\n",
    "    DF_joined = DF_prediction.join(churn_proportion, DF_prediction.prediction == churn_proportion.ClusterID, \"inner\")\n",
    "    #DF_joined.show(1)\n",
    "    #DF_joined.printSchema()\n",
    "    \n",
    "    # Rename columns\n",
    "    DF_prepared = DF_joined.withColumnRenamed(\"features\", \"featuresClustering\").withColumnRenamed(\"prediction\", \"predictionClustering\").withColumnRenamed(\"avg(CHURN)\", \"ClusterChurn\")\n",
    "    #DF_prepared.printSchema()\n",
    "    \n",
    "    return DF_prepared\n",
    "\n",
    "prep_df = preparation(df)\n",
    "# prep_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Assemble features for supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assembler_C = VectorAssembler(\n",
    "    inputCols=[\"SVC_CALLS\", \"DAY_MINS\", \"DAY_CHARGE\", \"VMAIL_MSGS\", \"VMAIL\", \n",
    "               \"INTL_CALLS\", \"INTL_CHARGE\", \"INTL_MINS\", \"EVE_CHARGE\", \"EVE_MINS\",\n",
    "               \"NIGHT_MINS\", \"NIGHT_CHARGE\", \"ClusterChurn\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "assembled_df = assembler_C.transform(prep_df)\n",
    "\n",
    "# Select only the features columns\n",
    "labeled_df = assembled_df.select(assembled_df[\"features\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Prediction__\n",
    "\n",
    "Test all models and compare results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|[4.0,129.1,21.95,...|[0.80710433645923...|[0.69149211293988...|       0.0|\n",
      "|[1.0,265.1,45.07,...|[0.36388357908790...|[0.58998021289068...|       0.0|\n",
      "|[4.0,332.9,56.59,...|[-1.2394847237748...|[0.22452568975126...|       1.0|\n",
      "|[1.0,161.6,27.47,...|[2.75909674548468...|[0.94042504861497...|       0.0|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "Random Forest\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|[4.0,129.1,21.95,...|[1.95945945945945...|[0.09797297297297...|       1.0|\n",
      "|[1.0,265.1,45.07,...|          [19.0,1.0]|         [0.95,0.05]|       0.0|\n",
      "|[4.0,332.9,56.59,...|          [2.0,18.0]|           [0.1,0.9]|       1.0|\n",
      "|[1.0,161.6,27.47,...|[17.9745207050461...|[0.89872603525230...|       0.0|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "Gradient Boosted Trees\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|[4.0,129.1,21.95,...|[-1.2363311499107...|[0.07779701456463...|       1.0|\n",
      "|[1.0,265.1,45.07,...|[1.51980594808876...|[0.95433191765956...|       0.0|\n",
      "|[4.0,332.9,56.59,...|[-1.6567604898101...|[0.03511024172892...|       1.0|\n",
      "|[1.0,161.6,27.47,...|[1.09860246721926...|[0.89999823212531...|       0.0|\n",
      "+--------------------+--------------------+--------------------+----------+"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "lr_pred = lrModel.transform(labeled_df)\n",
    "# Random Forest\n",
    "rf_pred = rfModel.transform(labeled_df)\n",
    "# GBT\n",
    "gbt_pred = gbtModel.transform(labeled_df)\n",
    "\n",
    "print(\"Logistic Regression\")\n",
    "lr_pred.show()\n",
    "print(\"Random Forest\")\n",
    "rf_pred.show()\n",
    "print(\"Gradient Boosted Trees\")\n",
    "gbt_pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Comments__\n",
    "\n",
    "By comparing the probabilities and predictions of the 3 models, we conclude that the first item is likely to have been misclassified by logistic regression. Our final prediction vector is [1,0,1,0]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you've learned\n",
    "\n",
    "Congratulations!\n",
    "\n",
    "In this notebook, you've seen how to:\n",
    "* load models into a Jupyter notebook\n",
    "* load data you had saved in Db2\n",
    "* deploy models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## Authors\n",
    "\n",
    "Eva Feillet - ML intern, IBM Cloud and Cognitive Software, IBM Lab in Böbligen, Germany"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
