{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IoT Streaming Analytics Demo\n",
    "This is a demo notebook for dashDB's integrated Apache Spark environment and how it can be used to process and land streaming data in dashDB tables.\n",
    "\n",
    "<img src=\"https://ibm.box.com/shared/static/6dr6gou6xr1alfwnj6zkj0tbsyc64vda.png\", width=700>\n",
    "\n",
    "If you are new to dashDB and it's integrated Apache Spark capabilities you may first want to check out this [overview article](http://www.ibmbigdatahub.com/blog/evolving-enterprise-data-warehouse-beyond-sql-apache-spark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up IoT demo data producer\n",
    "This demo relies on a little data producer container that you have to set up in addition to your dashDB local and Jupyter notebook containers on the same host machine. You can find it [here](https://github.com/ibmdbanalytics/dashdb_analytic_tools/tree/notebook-dev/dashdblocal_notebooks/iot_producer) in a sub folder of the dashDB Jupyter notebook container project. Please follow the instructions found there and perform the few simple steps to set it up. \n",
    "\n",
    "This producer will set up a Kafka server on port `9092` with a Kafka topic named `iot4dashdb` and permanently flow in there messages about wind turbine device measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming environment setup for Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the kafka streaming library in the classpath. Loading it via `%AddDep` magic does not currently work because of classloader issues (Spark datasource lookup doesn't find it). The dashDB container gets the kafka library deployed in the subsequent step. So adding the dependency is not necessary to execute the code. However the goal of this notebook is to eventually export, compile and deploy it as a Spark application inside dashDB. The compilation is going to be performed in the Jupyter container. For this reason we still need to add the kafka library as a compile-time dependency inside comments. The deployment function treats commented %AddDeps as a compile-only dependecy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//%AddDeps org.apache.spark spark-streaming-kafka-0-10-assembly_2.11 2.3.2\n",
    "true // to avoid a syntax error because of an empty code cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the Spark runtime for kafka streaming inside dashDB, we need to download the kafka assembly explicitly and place it in the spark/defaultlibs directory for the current user. This is done in the following cell. When you do this for the first time in your user's envuironment, you need to restart the kernel in order to make it effective in the classpath. You can do this by selecting `File->Close and Halt` and then opening this notebook again.\n",
    "\n",
    "Attention Any .jar in $HOME/spark/defaultlibs will be placed into the classpath for **all** Spark apps submitted\n",
    "by the user. With large assemblies like kafka, which contain widely used packages, e.g. jackson or apache commons, \n",
    "this may override classes that your applciation expects, so be careful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//NOT-FOR-APP\n",
    "val path = \"spark/defaultlibs\"\n",
    "val kafka_assembly = \"spark-streaming-kafka-0-10-assembly_2.11-2.3.2.jar\"\n",
    "val url = \"http://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10-assembly_2.11/2.3.2/\" + kafka_assembly\n",
    "if (new java.io.File(path + \"/\" + kafka_assembly).exists()) {\n",
    "    print(\"found kafka assembly\")\n",
    "} else {\n",
    "    import scala.sys.process._\n",
    "    s\"wget -nv -P $path $url\" ! ;\n",
    "    print(\"loaded kafka assembly, please restart the notebook\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the imports we need for our demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.kafka.clients.consumer.ConsumerRecord\n",
    "import org.apache.kafka.common.serialization.StringDeserializer\n",
    "import org.apache.spark.annotation.DeveloperApi\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types.LongType\n",
    "import org.apache.spark.sql.types.StringType\n",
    "import org.apache.spark.sql.types.StructType\n",
    "import org.apache.spark.sql.types.TimestampType\n",
    "import org.apache.spark.streaming.Durations\n",
    "import org.apache.spark.streaming.StreamingContext\n",
    "import org.apache.spark.streaming.dstream.DStream\n",
    "import org.apache.spark.streaming.kafka010.ConsumerStrategies\n",
    "import org.apache.spark.streaming.kafka010.KafkaUtils\n",
    "import org.apache.spark.streaming.kafka010.LocationStrategies\n",
    "import org.apache.spark.sql.SaveMode\n",
    "\n",
    "// import $ notation for Spark SQL\n",
    "val sparkImplicits = spark.implicits\n",
    "import sparkImplicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kafka configuration to connect to our IoT demo data producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val brokers = \"localhost:9092\"\n",
    "val topics = \"iot4dashdb\"\n",
    "val topicList = topics.split(\",\")\n",
    "\n",
    "val kafkaParams = Map[String, Object] (\n",
    "    \"bootstrap.servers\" -> brokers,\n",
    "    \"key.deserializer\" -> classOf[StringDeserializer],\n",
    "    \"value.deserializer\" -> classOf[StringDeserializer],\n",
    "    \"group.id\" -> \"democonsumer\",\n",
    "    \"auto.offset.reset\" -> \"latest\",\n",
    "    \"enable.auto.commit\" -> (false: java.lang.Boolean)\n",
    ")\n",
    "\n",
    "type inputStream = DStream[ConsumerRecord[String,String]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we define a utility function to set up a Kafka Streaming context, and run it for an optionally defined period of timeoutMillisecs with a given processing function. When we run this interactively in a notebook cell, it's important to set the timeout to to a specific value stop the streaming context again. Otherwise it will keep running and produce cell output, even after the cell has finished executing.\n",
    "\n",
    "Note that a streaming context cannot be re-used after it has executed, so we can't keep it across cells and therefore we have to use the utility function to re-create it before every execution.\n",
    "\n",
    "We use a finally block to make sure the context is stopped even in case of exception; otherwise the whole streaming infrastructure becomes unusable within the notebook kernels lifetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var timeoutMillisecs: Option[Long] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def runStreamingContext(processMessages: inputStream => Unit): Unit = {\n",
    "    val ssc = new StreamingContext(spark.sparkContext, Durations.seconds(3))\n",
    "    val messages = KafkaUtils.createDirectStream(\n",
    "                ssc,\n",
    "                LocationStrategies.PreferConsistent,\n",
    "                ConsumerStrategies.Subscribe[String,String](topicList, kafkaParams)\n",
    "            );\n",
    "    processMessages(messages)\n",
    "    \n",
    "    try {\n",
    "        ssc.start()\n",
    "        timeoutMillisecs match {\n",
    "            case Some(t) => ssc.awaitTerminationOrTimeout(t)\n",
    "            case _ => ssc.awaitTermination()\n",
    "        }\n",
    "    } finally {\n",
    "        ssc.stop(false, true)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running interactively in the notebook we want all streaming tests to run only for 10 seconds and stop automatically. Later, when we deploy the notebook as a stand-alone Spark application, we want the processing to continue until the application is stopped externally. Hence we tag the cell NOT-FOR-APP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//NOT-FOR-APP\n",
    "timeoutMillisecs = Some(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we simply iterate over the batch RDDs and print out their content, just to explore the data in our Kafka queue. We also don't need this when we deploy the notebook as application. So we tag the cell accordingly.\n",
    "\n",
    "Before you execute the cell below, you will need to start the producer container that writes into the kafka queue. When the IoT producer container is not started or has already terminated, the streaming computation in the following cell will hang and never return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//NOT-FOR-APP\n",
    "runStreamingContext(messages =>\n",
    "  // The ConsumerRecord objects produced by a Kafa queue are not serializable, need to map them to a String\n",
    "  // before collecting\n",
    "  messages.map(record => s\"${record.key} -> ${record.value}\").\n",
    "    foreachRDD(rdd => rdd.collect().foreach(println))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this gives us an idea about the JSON format we can expect from our data source. Lets try to parse each batch into a DataSet with Spark SQL. We don't want Spark to perform automatic JSON schema detection over and over again for each batch, so we give it a pre-defined schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val schema = (new StructType).\n",
    "    add(\"payload\", (new StructType).\n",
    "        add(\"temperature\", LongType).\n",
    "        add(\"tempOutside\", LongType).\n",
    "        add(\"powerProd\", LongType).\n",
    "        add(\"noiseLevel1\", LongType).\n",
    "        add(\"time\", TimestampType)).\n",
    "    add(\"deviceId\", StringType).\n",
    "    add(\"deviceType\", StringType).\n",
    "    add(\"eventType\", StringType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the produced data has a nested structure. \n",
    "Spark can deal with this type of schema, but it's not suitable being stored in a plain SQL database table, so we flatten it to straight rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//NOT-FOR-APP\n",
    "def processStream1(messages: inputStream): Unit = {\n",
    "    messages.map(_.value).foreachRDD(rdd => {\n",
    "        val ds = spark.read.schema(schema).json(rdd)\n",
    "        val flatDataset = ds.select($\"payload.temperature\", $\"payload.tempOutside\",\n",
    "            $\"payload.powerProd\", $\"payload.noiseLevel1\", $\"payload.time\",\n",
    "            $\"deviceId\", $\"deviceType\", $\"eventType\")\n",
    "        flatDataset.select(\"deviceId\", \"time\", \"temperature\", \"tempOutside\", \"powerProd\", \"noiseLevel1\").show(false)\n",
    "    })\n",
    "}\n",
    "runStreamingContext(processStream1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write IoT records to a persistent table\n",
    "In the desired deployed application we want to write the records read from the queue into a dashDB table instead of only printing them out the datasets. In this demonstration here we will always first empty the table in the first batch after the application is started. We use Spark's 'Overwrite' mode for writing the first batch to clear a pre-existing table and then use 'Append' mode in subsequent batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val tableName = \"USERDATA.IOT_EVENTS\"\n",
    "\n",
    "var saveMode = SaveMode.Overwrite\n",
    "def processStream2(messages: inputStream): Unit = {\n",
    "    messages.map(_.value).foreachRDD(rdd => {\n",
    "        val ds = spark.read.schema(schema).json(rdd)\n",
    "        val flatDataset = ds.select($\"payload.temperature\", $\"payload.tempOutside\",\n",
    "            $\"payload.powerProd\", $\"payload.noiseLevel1\", $\"payload.time\",\n",
    "            $\"deviceId\", $\"deviceType\", $\"eventType\")\n",
    "        flatDataset.write.format(\"com.ibm.idax.spark.idaxsource\").\n",
    "            option(\"dbtable\", tableName).\n",
    "            option(\"allowAppend\", \"TRUE\").\n",
    "            mode(saveMode).\n",
    "            save()\n",
    "        println(\"Batch written to database\")\n",
    "        saveMode = SaveMode.Append\n",
    "    })\n",
    "}\n",
    "runStreamingContext(processStream2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you want to capture some longer timeframes of events in this notebook, increase the timeoutMillisecs e.g. by running the cell below and then running the previous cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//NOT-FOR-APP\n",
    "timeoutMillisecs = Some(120000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying automatic background IoT landing app inside dashDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally you can deploy the essential parts of this notebook (i.e. those **not** marked with //NOT-FOR-APP) as a stand-alone scala application into dashDB. To do this, select `File -> Deploy as -> Deploy to dashDB Spark` from the menu. Then use one of the alternatives shown in the result page to launch the application, e.g. using the [SPARK_SUBMIT](https://www.ibm.com/support/knowledgecenter/SS6NHC/com.ibm.swg.im.dashdb.analytics.doc/doc/r_spark_applications_spark_submit.html) stored procedure. \n",
    "\n",
    "If you want to take a look at the generated code, select `File -> Download as -> Scala class (in browser)`\n",
    "\n",
    "This notebook is set up so that the timeout does not apply in the exported application, so your streaming context will continue to run and insert events into the database until the application is explicitly stopped via [spark-submit.sh --kill](https://www.ibm.com/support/knowledgecenter/SS6NHC/com.ibm.swg.im.dashdb.doc/learn_how/spark_ref.html), the \n",
    "[CANCEL_APP](https://www.ibm.com/support/knowledgecenter/SS6NHC/com.ibm.swg.im.dashdb.analytics.doc/doc/r_spark_applications_cancel_app.html) stored procedure, the [/public/apps/cancel](https://developer.ibm.com/clouddataservices/wp-content/themes/projectnext-clouddata/dashDBanalytics/#/) endpoint of dashDB's REST API or the Spark monitoring UI in the dashDB console (`Monitor->Workloads->Spark`).\n",
    "\n",
    "**For your convenience** you can simply use the cells in the next section to start and stop the deployed app in the background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launching deployed IoD landing app in background\n",
    "Run the following cell to launch the just deployed IoD landing app inside dashDB. The cell keeps a database connection open that we will use to stop the landing app again when you execute the subsequent cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//NOT-FOR-APP\n",
    "import java.sql.DriverManager\n",
    "import java.sql.Connection\n",
    "import java.sql.SQLException\n",
    "val connection = DriverManager.getConnection(\"jdbc:db2:BLUDB\")\n",
    "var iot_lander_submission_id : String = null\n",
    "try {   \n",
    "    val sp_call = connection.prepareCall(\n",
    "      \"CALL IDAX.SPARK_SUBMIT(?, '{ \\\"appResource\\\" : \\\"IoT_demo-assembly-1.0.jar\\\", \" +\n",
    "                                    \"\\\"mainClass\\\" : \\\"SampleApp\\\"}', \" +\n",
    "                                    \"'mode=async')\")\n",
    "    sp_call.registerOutParameter(1, java.sql.Types.VARCHAR);\n",
    "    sp_call.executeUpdate();\n",
    "    iot_lander_submission_id = sp_call.getString(1)\n",
    "    println(\"Successfully launched IoT landing app with submission id \" + iot_lander_submission_id);\n",
    "} catch {\n",
    "    case e: SQLException => { println(\"Error: \" + e) }\n",
    "    iot_lander_submission_id = null\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the cells in the subsequent section to observe the progress of landing IoT messages in the target table. When you are done, you can run the following cell to stop the IoT landing app that you started in the background in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//NOT-FOR-APP\n",
    "if(iot_lander_submission_id !=null) {\n",
    "    try {   \n",
    "        val sp_call = connection.prepareCall(\"CALL IDAX.CANCEL_APP(?)\")\n",
    "        sp_call.setString(1, iot_lander_submission_id);\n",
    "        sp_call.executeUpdate();\n",
    "        println(\"Successfully stopped IoT landing app with submission id \" + iot_lander_submission_id)\n",
    "    } catch {\n",
    "        case e: SQLException => { println(\"Error: \" + e) }\n",
    "    }\n",
    "} else println(\"No submission ID defined\")\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check landed IoT data\n",
    "You can verify the landed data in dashDB. For this we simply establish a data frame on the result table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//NOT-FOR-APP\n",
    "var landed_iot_data = spark.read.\n",
    "   format(\"com.ibm.idax.spark.idaxsource\").\n",
    "   option(\"url\", \"jdbc:db2:BLUDB\").\n",
    "   option(\"dbtable\", tableName).\n",
    "   load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we show the content. You can verify the progress of your deployed landing app by running the next cell repeatetly and check the new records and overall count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//NOT-FOR-APP\n",
    "println(\"Total IoT records: \"+ landed_iot_data.count())\n",
    "println(\"Newest 10 IoT records:\")\n",
    "landed_iot_data.select(\"deviceId\", \"time\", \"temperature\", \"tempOutside\", \"powerProd\", \"noiseLevel1\").\n",
    "                orderBy(org.apache.spark.sql.functions.col(\"time\").desc).show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDAX - Scala",
   "language": "scala",
   "name": "idax-scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
