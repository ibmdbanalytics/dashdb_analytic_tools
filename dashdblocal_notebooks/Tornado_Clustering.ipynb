{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are new to dashDB and it's integrated Apache Spark capabilities you may first want to check out this [overview article](http://www.ibmbigdatahub.com/blog/evolving-enterprise-data-warehouse-beyond-sql-apache-spark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "/*\n",
    " * Â© Copyright IBM Corporation 2016 All rights reserved.\n",
    " * LICENSE: BSD-3, https://opensource.org/licenses/BSD-3-Clause\n",
    " */\n",
    "\n",
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.types.DoubleType\n",
    "import org.apache.spark.sql.types.StructType\n",
    "import org.apache.spark.sql.types.StructField\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types.IntegerType\n",
    "import java.sql.Connection\n",
    "import java.sql.SQLException\n",
    "import java.sql.ResultSet\n",
    "import java.sql.Statement\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.clustering.KMeans\n",
    "import org.apache.spark.ml.clustering.KMeansModel\n",
    "import org.apache.spark.ml.feature.VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the brunel visualitzation lib for Scala. <b>Note</b> that the brunelvis.org server is often not accessible from certain locations, e.g. from softlayer machines. In that case download the library by hand and upload to dashDB via __`spark-submit.sh --upload-files`__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//NOT-FOR-APP\n",
    "//Load the library directly from the repository:\n",
    "%AddJar -magic https://brunelvis.org/jar/spark-kernel-brunel-all-2.4.jar\n",
    "\n",
    "//Use the following lines (adapt user name) if access to repository is not working due to some local firewall constraints.\n",
    "//%AddJar -magic file:/mnt/blumeta0/home/bluadmin/spark/apps/spark-kernel-brunel-all-2.4.jar -f\n",
    "true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell prepares the input data based on sample tables that are available in every dashDB system out of the box. So you can run this notebook against any dashDB instance that you have available. But when the cell runs for the first time the setup can take a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//NOT-FOR-APP\n",
    "import java.sql.DriverManager\n",
    "import java.sql.Connection\n",
    "val connection = DriverManager.getConnection(\"jdbc:db2:BLUDB\")\n",
    "try {   \n",
    "    val statement = connection.createStatement()\n",
    "    statement.executeUpdate(\"create table tornado as ( \" +\n",
    "         \" select objectid,yr,mo,dy, time, cast(slat as float) as slat,cast(slon as float) as slon, \" +\n",
    "         \"        elat,elon,cast (len as float) as len,cast(wid as float) as wid,st,cast(mag as float) as mag \" +\n",
    "         \" from samples.geo_tornado ) with data\")\n",
    "} catch {\n",
    "    case e: SQLException => { if (e.getErrorCode() == -601) println(\"Table TORNADO already exists\") else throw e }\n",
    "}\n",
    "try {   \n",
    "    val statement = connection.createStatement()\n",
    "    statement.executeUpdate(\"create table texas_customers as ( \" +\n",
    "         \" select b.objectid, b.name, b.insurance_value, db2gse.st_x(b.shape) lon, db2gse.st_y(b.shape) lat \" +\n",
    "         \"     from samples.geo_county a, samples.geo_customer b \" +\n",
    "         \"     where a.statefp='48' and db2gse.st_within(b.shape, a.shape) = 1 \" +\n",
    "         \" ) with data\")\n",
    "} catch {\n",
    "    case e: SQLException => { if (e.getErrorCode() == -601) println(\"Table TEXAS_CUSTOMERS already exists\") else throw e }\n",
    "}\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the training data using the IDAX data source. In case of dashDB MPP systems this will give us\n",
    "automatically a DataFrame with exactly the number of partitions as there are MLNs (database partitions)\n",
    "in dashDB and the DataFrame partitions will be located in each according executor colocated with the\n",
    "dashDB MLN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var input = spark.read.\n",
    "   format(\"com.ibm.idax.spark.idaxsource\").\n",
    "   option(\"dbtable\", \"TORNADO\").\n",
    "   load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data are all recorded tornados in USA since 1950. It is taken from http://www.spc.noaa.gov/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When this demo is run with a dashDB SMP system (i.e. non partitioned databse) the next step should be run in order to achieve parallel computation. You should skip this step in case you run against a dashDB MPP because then the data frame is partitioned automatically for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// (4) Repartition to 10 partitions\n",
    "// not needed for MPP\n",
    "input = input.repartition(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the Tornado data now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//NOT-FOR-APP\n",
    "input.show()\n",
    "\n",
    "val density = input.select(\"ST\").groupBy(\"ST\").count()\n",
    "density.orderBy(org.apache.spark.sql.functions.col(\"count\").desc).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualization we sample the data down a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//NOT-FOR-APP\n",
    "val inputSample = input.sample(false,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize all sampled tornado start coordinates on a map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%brunel map('usa') + data('inputSample') x(slon) y(slat) title(\"Input Data Density per State\") :: width=800, height=400\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize tht as a density map summed up per state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%brunel map('usa') + data('density') map color(count:[blues, reds]) key(st) label(st) title(\"Input Data Density per State\")  :: width=800, height=800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on Texas as the state with highest density of tornados. So let's filter the tornado records for Texas only and see how many we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val input_texas = input.filter(\"ST = 'TX'\")\n",
    "input_texas.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "Let's use machine learning to cluster the hot spots for tornados within Texas. We use a classical clustering ML algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the feature selection stage of the Spark ML pipeline. We train based on Tornado latitude and logitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val assembler = new VectorAssembler().\n",
    "            setInputCols(Array(\"SLAT\", \"SLON\")).\n",
    "            setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the model training stage for unsupervised cluster training using KMeans algorithm from Spark ML API. You can change the number of iterations as well as the number of cluster centers that should be detected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val clustering = new KMeans().\n",
    "            setFeaturesCol(\"features\").\n",
    "            setK(5).\n",
    "            setMaxIter(5).\n",
    "            setSeed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we stitch together the ML pipeline with feature selection and training stages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val pipe = new Pipeline().setStages(Array(assembler, clustering))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the ML pipeline on the input data of Texas tornados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val model = pipe.fit(input_texas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ran quick. If you want to observe a longer model training run, execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var input_ballooned = input.unionAll(input).unionAll(input).unionAll(input).unionAll(input).unionAll(input).unionAll(input)\n",
    "pipe.fit(input_ballooned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the trained model and print out the detected cluster centers with their latitude and lognitude:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//NOT-FOR-APP\n",
    "val clustercenters = model.stages(1).asInstanceOf[KMeansModel].clusterCenters\n",
    "val clustercentersschema = StructType(for (i <- 1 to clustercenters.head.size) yield StructField(s\"col$i\", DoubleType, false))\n",
    "val clustercentersconverted = clustercenters.map { x: Vector => Row.fromSeq(x.toArray) }\n",
    "val clustercentersdataframe = spark.createDataFrame(sc.parallelize(clustercentersconverted), clustercentersschema).toDF(\"SLAT\", \"SLON\")\n",
    "clustercentersdataframe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot that on a map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%brunel map('usa') + data('clustercentersdataframe') x(slon) y(slat)  title(\"Tornado Cluster Centers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing to official NOAA Tornado density maps confirms that our cluster centers make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](http://www.spc.noaa.gov/wcm/2015-wbc-anoms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test the model by simply assigning (scoring) the clusters for each recorded tornado in Texas and then aggregating and ranking each cluster by the number of assigned tornados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//NOT-FOR-APP\n",
    "val output_train = model.transform(input_texas).select(\"OBJECTID\",\"SLON\",\"SLAT\",\"prediction\")\n",
    "val tornados_with_clusters = output_train.toDF(Seq(\"OBJECTID\",\"SLON\",\"SLAT\",\"ClusterID\"): _*)\n",
    "\n",
    "tornados_with_clusters.groupBy(\"ClusterID\").count().orderBy(org.apache.spark.sql.functions.col(\"count\").desc).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making some real predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to assign each holder of an insurance policies for real estate in Texas to one of the tornado clusters in order to caclulate a risk score according to which cluster his property belongs to.\n",
    "\n",
    "First we read in insurance holder records for Texas (this is mock up data for the sake of this demo notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var insurance_holders = spark.read.\n",
    "   format(\"com.ibm.idax.spark.idaxsource\").\n",
    "   option(\"dbtable\", \"TEXAS_CUSTOMERS\").\n",
    "   load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again run the following cell only when you are on a dashDB SMP system to get a partitioned data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "insurance_holders = insurance_holders.repartition(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the insurance holder data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//NOT-FOR-APP\n",
    "var insurance_holders_sample = insurance_holders.sample(false,0.05)\n",
    "insurance_holders.show()\n",
    "insurance_holders.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize a sample of the Texas insurance holders on a map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%brunel map('usa') + data('insurance_holders_sample') x(lon) y(lat) title(\"Customers in Texas\") :: width=800, height=400 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we run a prediction with the tornado cluster model on the insurance holders to assign each one to its according tornado cluster. Then we print a sample of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val insurance_holders_renamed = insurance_holders.toDF(Seq(\"OBJECTID\", \"NAME\", \"INSURANCE_VALUE\", \"SLON\", \"SLAT\"): _*)\n",
    "var insurance_holders_scored = model.transform(insurance_holders_renamed).select(\"OBJECTID\", \"NAME\", \"INSURANCE_VALUE\", \"SLON\",\"SLAT\",\"prediction\")\n",
    "insurance_holders_scored = insurance_holders_scored.toDF(Seq(\"OBJECTID\", \"NAME\", \"INSURANCE_VALUE\", \"SLON\", \"SLAT\", \"ClusterID\"): _*)\n",
    "\n",
    "insurance_holders_scored.show(10)\n",
    "//insurance_holders_scored.groupBy(\"ClusterID\").count().show()\n",
    "\n",
    "println(\"Writing results to table TEXAS_CUSTOMERS_SCORED ...\")\n",
    "insurance_holders_scored.write.format(\"com.ibm.idax.spark.idaxsource\").\n",
    "   option(\"dbtable\", \"TEXAS_CUSTOMERS_SCORED\").\n",
    "   mode(SaveMode.Overwrite).\n",
    "   save()\n",
    "println(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting cluster ID determines the risk class that the insurance holder will be assigned to. In addition we could run a fine tuning for the risk assignment by folding in the distance from the insured property to the cluster center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's colorize the sampled Texas insurance holders according to their assigned cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//NOT-FOR-APP\n",
    "var insurance_holders_scored_sample = insurance_holders_scored.sample(false, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%brunel map('usa') + data('insurance_holders_scored_sample') x(SLON) y(SLAT) title(\"Texas Customers - Cluster-Relation\") color(ClusterID) :: width=800, height=400 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying as application\n",
    "You can easily deploy the essential logic of this notebook as an application that can be run at any time via a dashDB [stored procedure](https://www.ibm.com/support/knowledgecenter/SS6NHC/com.ibm.swg.im.dashdb.analytics.doc/doc/r_spark_applications_functions.html) invokation, via dashDB's command line [spark-submit.sh](https://www.ibm.com/support/knowledgecenter/SS6NHC/com.ibm.swg.im.dashdb.doc/learn_how/spark_ref.html) or via a [REST API](https://developer.ibm.com/clouddataservices/wp-content/themes/projectnext-clouddata/dashDBanalytics/#/).\n",
    "\n",
    "Note that a few cells above have the __`//NOT-FOR-APP`__ comment. Cells with this comment as well as cells that only contain Jupyter cell magic starting with __`%%`__ are automatically skipped when deploying the notebook as an application. This way you can organize your notebook for interactive vs. backend logic.\n",
    "\n",
    "In order to generate and deploy an application for this notebook select __`File->Deploy as->Deploy to dashDB Spark`__ in the menu above. After a short while it shows you the result page for the deployment with the specific invocation options that you just can copy and past to try them out immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDAX - Scala",
   "language": "scala",
   "name": "idax-scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
