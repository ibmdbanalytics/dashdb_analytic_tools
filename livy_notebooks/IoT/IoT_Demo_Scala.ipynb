{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IoT Streaming Analytics Demo\n",
    "This is a demo notebook for Db2 Warehouse's integrated Apache Spark environment and how it can be used to process and land streaming data in Db2 tables. It runs through livy with a Spark kernel.\n",
    "\n",
    "This notebook also uses ibmdbpy. If you are new to ibmdbpy, please refer to this [folder](https://github.com/ibmdbanalytics/ibmdbpy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://ibm.box.com/shared/static/6dr6gou6xr1alfwnj6zkj0tbsyc64vda.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up IoT demo data producer\n",
    "This demo relies on a little data producer container that you have to set up in addition to your dashDB local and Jupyter notebook containers on the same host machine. You can find it [here](https://github.com/ibmdbanalytics/dashdb_analytic_tools/tree/notebook-dev/dashdblocal_notebooks/iot_producer) in a sub folder of the dashDB Jupyter notebook container project. Please follow the instructions found there and perform the few simple steps to set it up. \n",
    "\n",
    "This producer will set up a Kafka server on port `9092` with a Kafka topic named `iot4dashdb` and permanently flow in there messages about wind turbine device measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming environment setup for Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>221</td><td>None</td><td>spark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res3: Boolean = true\n"
     ]
    }
   ],
   "source": [
    "//%AddDeps org.apache.spark spark-streaming-kafka-0-10-assembly_2.11 2.0.2 before\n",
    "// We use org.apache.spark spark-streaming-kafka-0-8-assembly_2.11 2.3.0 here --> still the same error\n",
    "true // to avoid a syntax error because of an empty code cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Kafka assembly__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if you have Kafka assembly available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: String = /mnt/blumeta0/home/bluadmin/spark/defaultlibs\n",
      "kafka_assembly: String = spark-streaming-kafka-0-10-assembly_2.11-2.3.0.jar\n",
      "url: String = http://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10-assembly_2.11/2.3.0/spark-streaming-kafka-0-10-assembly_2.11-2.3.0.jar\n",
      "warning: there was one feature warning; re-run with -feature for details\n",
      "kafka assembly found"
     ]
    }
   ],
   "source": [
    "val path = System.getenv(\"HOME\") + \"/spark/defaultlibs\"\n",
    "val kafka_assembly = \"spark-streaming-kafka-0-10-assembly_2.11-2.3.0.jar\"\n",
    "val url = \"http://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10-assembly_2.11/2.3.0/\" + kafka_assembly\n",
    "if (new java.io.File(path + \"/\" + kafka_assembly).exists()) {\n",
    "    print(\"kafka assembly found\")\n",
    "} else {\n",
    "    import scala.sys.process._\n",
    "    s\"wget -nv -P $path $url\" ! ;\n",
    "    print(\"loaded kafka assembly, please restart the notebook\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Imports__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.kafka.clients.consumer.ConsumerRecord\n",
      "import org.apache.kafka.common.serialization.StringDeserializer\n",
      "import org.apache.spark.annotation.DeveloperApi\n",
      "import org.apache.spark.sql.SQLContext\n",
      "import org.apache.spark.sql.SparkSession\n",
      "import org.apache.spark.sql.types.LongType\n",
      "import org.apache.spark.sql.types.StringType\n",
      "import org.apache.spark.sql.types.StructType\n",
      "import org.apache.spark.sql.types.TimestampType\n",
      "import org.apache.spark.streaming.Durations\n",
      "import org.apache.spark.streaming.StreamingContext\n",
      "import org.apache.spark.streaming.dstream.DStream\n",
      "import org.apache.spark.streaming.kafka010.ConsumerStrategies\n",
      "import org.apache.spark.streaming.kafka010.KafkaUtils\n",
      "import org.apache.spark.streaming.kafka010.LocationStrategies\n",
      "import org.apache.spark.sql.SaveMode\n",
      "sparkImplicits: spark.implicits.type = org.apache.spark.sql.SparkSession$implicits$@d555969\n",
      "import sparkImplicits._\n"
     ]
    }
   ],
   "source": [
    "import org.apache.kafka.clients.consumer.ConsumerRecord\n",
    "import org.apache.kafka.common.serialization.StringDeserializer\n",
    "import org.apache.spark.annotation.DeveloperApi\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types.LongType\n",
    "import org.apache.spark.sql.types.StringType\n",
    "import org.apache.spark.sql.types.StructType\n",
    "import org.apache.spark.sql.types.TimestampType\n",
    "import org.apache.spark.streaming.Durations\n",
    "import org.apache.spark.streaming.StreamingContext\n",
    "import org.apache.spark.streaming.dstream.DStream\n",
    "import org.apache.spark.streaming.kafka010.ConsumerStrategies\n",
    "import org.apache.spark.streaming.kafka010.KafkaUtils\n",
    "import org.apache.spark.streaming.kafka010.LocationStrategies\n",
    "import org.apache.spark.sql.SaveMode\n",
    "\n",
    "// import $ notation for Spark SQL\n",
    "val sparkImplicits = spark.implicits\n",
    "import sparkImplicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local visualisations will use pandas and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%local\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#Useful imports for local visualisations\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the Spark runtime for kafka streaming inside dashDB, we need to download the kafka assembly explicitly and place it in the spark/defaultlibs directory for the current user. This is done in the following cell. When you do this for the first time in your user's envuironment, you need to restart the kernel in order to make it effective in the classpath. You can do this by selecting `File->Close and Halt` and then opening this notebook again.\n",
    "\n",
    "Attention: Any .jar in $HOME/spark/defaultlibs will be placed into the classpath for **all** Spark apps submitted\n",
    "by the user. With large assemblies like kafka, which contain widely used packages, e.g. jackson or apache commons, \n",
    "this may override classes that your applciation expects, so be careful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Kafka configuration to connect to our IoT demo data producer__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a utility function to set up a Kafka Streaming context, and run it for an optionally defined period of timeoutMillisecs with a given processing function. \n",
    "\n",
    "**Important note** : When we run this interactively in a notebook cell, it's important to set the timeout to a specific value in order to stop the streaming context. Otherwise it will keep running and produce cell output, even after the cell has finished executing.\n",
    "\n",
    "Note also that a streaming context cannot be re-used after it has executed, so we can't keep it across cells and therefore we have to use the utility function to re-create it before every execution.\n",
    "\n",
    "We use a `finally` block to make sure the context is stopped even in case of exception; otherwise the whole streaming infrastructure becomes unusable within the notebook kernels lifetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brokers: String = localhost:9092\n",
      "topics: String = iot4dashdb\n",
      "topicList: Array[String] = Array(iot4dashdb)\n",
      "kafkaParams: scala.collection.immutable.Map[String,Object] = Map(key.deserializer -> class org.apache.kafka.common.serialization.StringDeserializer, auto.offset.reset -> latest, group.id -> democonsumer, bootstrap.servers -> localhost:9092, enable.auto.commit -> false, value.deserializer -> class org.apache.kafka.common.serialization.StringDeserializer)\n",
      "defined type alias inputStream\n"
     ]
    }
   ],
   "source": [
    "val brokers = \"localhost:9092\"\n",
    "val topics = \"iot4dashdb\"\n",
    "val topicList = topics.split(\",\")\n",
    "\n",
    "val kafkaParams = Map[String, Object] (\n",
    "    \"bootstrap.servers\" -> brokers,\n",
    "    \"key.deserializer\" -> classOf[StringDeserializer],\n",
    "    \"value.deserializer\" -> classOf[StringDeserializer],\n",
    "    \"group.id\" -> \"democonsumer\",\n",
    "    \"auto.offset.reset\" -> \"latest\",\n",
    "    \"enable.auto.commit\" -> (false: java.lang.Boolean)\n",
    ")\n",
    "\n",
    "type inputStream = DStream[ConsumerRecord[String,String]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeoutMillisecs: Option[Long] = None\n"
     ]
    }
   ],
   "source": [
    "var timeoutMillisecs: Option[Long] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runStreamingContext: (processMessages: inputStream => Unit)Unit\n"
     ]
    }
   ],
   "source": [
    "def runStreamingContext(processMessages: inputStream => Unit): Unit = {\n",
    "    val ssc = new StreamingContext(spark.sparkContext, Durations.seconds(3))\n",
    "    val messages = KafkaUtils.createDirectStream(\n",
    "                ssc,\n",
    "                LocationStrategies.PreferConsistent,\n",
    "                ConsumerStrategies.Subscribe[String,String](topicList, kafkaParams)\n",
    "            );\n",
    "    processMessages(messages)\n",
    "    \n",
    "    try {\n",
    "        ssc.start()\n",
    "        timeoutMillisecs match {\n",
    "            case Some(t) => ssc.awaitTerminationOrTimeout(t)\n",
    "            case _ => ssc.awaitTermination()\n",
    "        }\n",
    "    } finally {\n",
    "        ssc.stop(false, true)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running interactively in the notebook we want all streaming tests to run only for 10 seconds and stop automatically. Later, when we deploy the notebook as a stand-alone Spark application, we want the processing to continue until the application is stopped externally. Hence we tag the cell NOT-FOR-APP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeoutMillisecs: Option[Long] = Some(10000)\n"
     ]
    }
   ],
   "source": [
    "timeoutMillisecs = Some(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we simply iterate over the batch RDDs and print out their content, just to explore the data in our Kafka queue. We also don't need this when we deploy the notebook as application. So we tag the cell accordingly.\n",
    "\n",
    "**Important note**: Before you execute the cell below, you will need to start the producer container that writes into the kafka queue. When the IoT producer container is not started or has already terminated, the streaming computation in the following cell will hang and never return.\n",
    "\n",
    "Please refer to [this document](https://github.com/ibmdbanalytics/dashdb_analytic_tools/blob/livy-demos/dashdblocal_notebooks/iot_producer/README.md) to start the producer container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "org.apache.kafka.common.config.ConfigException: Missing required configuration \"partition.assignment.strategy\" which has no default value.\n",
      "  at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:124)\n",
      "  at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:48)\n",
      "  at org.apache.kafka.clients.consumer.ConsumerConfig.<init>(ConsumerConfig.java:194)\n",
      "  at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:380)\n",
      "  at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:363)\n",
      "  at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:350)\n",
      "  at org.apache.spark.streaming.kafka010.Subscribe.onStart(ConsumerStrategy.scala:84)\n",
      "  at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.consumer(DirectKafkaInputDStream.scala:70)\n",
      "  at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.start(DirectKafkaInputDStream.scala:240)\n",
      "  at org.apache.spark.streaming.DStreamGraph$$anonfun$start$7.apply(DStreamGraph.scala:54)\n",
      "  at org.apache.spark.streaming.DStreamGraph$$anonfun$start$7.apply(DStreamGraph.scala:54)\n",
      "  at scala.collection.parallel.mutable.ParArray$ParArrayIterator.foreach_quick(ParArray.scala:143)\n",
      "  at scala.collection.parallel.mutable.ParArray$ParArrayIterator.foreach(ParArray.scala:136)\n",
      "  at scala.collection.parallel.ParIterableLike$Foreach.leaf(ParIterableLike.scala:972)\n",
      "  at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)\n",
      "  at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\n",
      "  at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\n",
      "  at scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)\n",
      "  at scala.collection.parallel.ParIterableLike$Foreach.tryLeaf(ParIterableLike.scala:969)\n",
      "  at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)\n",
      "  at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)\n",
      "  at scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)\n",
      "  at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n",
      "  at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n",
      "  at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n",
      "  at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n",
      "  at ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()\n",
      "  at org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:578)\n",
      "  at org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:572)\n",
      "  at runStreamingContext(<console>:62)\n",
      "  ... 52 elided\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runStreamingContext(messages =>\n",
    "  // The ConsumerRecord objects produced by a Kafa queue are not serializable, need to map them to a String\n",
    "  // before collecting\n",
    "  messages.map(record => s\"${record.key} -> ${record.value}\").\n",
    "    foreachRDD(rdd => rdd.collect().foreach(println))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this gives us an idea about the JSON format we can expect from our data source. Lets try to parse each batch into a DataSet with Spark SQL. We don't want Spark to perform automatic JSON schema detection over and over again for each batch, so we give it a pre-defined schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schema: org.apache.spark.sql.types.StructType = StructType(StructField(payload,StructType(StructField(temperature,LongType,true), StructField(tempOutside,LongType,true), StructField(powerProd,LongType,true), StructField(noiseLevel1,LongType,true), StructField(time,TimestampType,true)),true), StructField(deviceId,StringType,true), StructField(deviceType,StringType,true), StructField(eventType,StringType,true))\n"
     ]
    }
   ],
   "source": [
    "val schema = (new StructType).\n",
    "    add(\"payload\", (new StructType).\n",
    "        add(\"temperature\", LongType).\n",
    "        add(\"tempOutside\", LongType).\n",
    "        add(\"powerProd\", LongType).\n",
    "        add(\"noiseLevel1\", LongType).\n",
    "        add(\"time\", TimestampType)).\n",
    "    add(\"deviceId\", StringType).\n",
    "    add(\"deviceType\", StringType).\n",
    "    add(\"eventType\", StringType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the produced data has a nested structure. \n",
    "Spark can deal with this type of schema, but it's not suitable being stored in a plain SQL database table, so we flatten it to straight rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "org.apache.kafka.common.config.ConfigException: Missing required configuration \"partition.assignment.strategy\" which has no default value.\n",
      "  at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:124)\n",
      "  at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:48)\n",
      "  at org.apache.kafka.clients.consumer.ConsumerConfig.<init>(ConsumerConfig.java:194)\n",
      "  at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:380)\n",
      "  at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:363)\n",
      "  at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:350)\n",
      "  at org.apache.spark.streaming.kafka010.Subscribe.onStart(ConsumerStrategy.scala:84)\n",
      "  at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.consumer(DirectKafkaInputDStream.scala:70)\n",
      "  at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.start(DirectKafkaInputDStream.scala:240)\n",
      "  at org.apache.spark.streaming.DStreamGraph$$anonfun$start$7.apply(DStreamGraph.scala:54)\n",
      "  at org.apache.spark.streaming.DStreamGraph$$anonfun$start$7.apply(DStreamGraph.scala:54)\n",
      "  at scala.collection.parallel.mutable.ParArray$ParArrayIterator.foreach_quick(ParArray.scala:143)\n",
      "  at scala.collection.parallel.mutable.ParArray$ParArrayIterator.foreach(ParArray.scala:136)\n",
      "  at scala.collection.parallel.ParIterableLike$Foreach.leaf(ParIterableLike.scala:972)\n",
      "  at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)\n",
      "  at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\n",
      "  at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\n",
      "  at scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)\n",
      "  at scala.collection.parallel.ParIterableLike$Foreach.tryLeaf(ParIterableLike.scala:969)\n",
      "  at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)\n",
      "  at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)\n",
      "  at scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)\n",
      "  at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n",
      "  at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n",
      "  at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n",
      "  at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n",
      "  at ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()\n",
      "  at org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:578)\n",
      "  at org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:572)\n",
      "  at runStreamingContext(<console>:62)\n",
      "  ... 52 elided\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def processStream1(messages: inputStream): Unit = {\n",
    "    messages.map(_.value).foreachRDD(rdd => {\n",
    "        val ds = spark.read.schema(schema).json(rdd)\n",
    "        val flatDataset = ds.select($\"payload.temperature\", $\"payload.tempOutside\",\n",
    "            $\"payload.powerProd\", $\"payload.noiseLevel1\", $\"payload.time\",\n",
    "            $\"deviceId\", $\"deviceType\", $\"eventType\")\n",
    "        flatDataset.select(\"deviceId\", \"time\", \"temperature\", \"tempOutside\", \"powerProd\", \"noiseLevel1\").show(false)\n",
    "    })\n",
    "}\n",
    "runStreamingContext(processStream1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write IoT records to a persistent table\n",
    "\n",
    "In the desired deployed application we want to write the records read from the queue into a Db2 table instead of only printing them out the datasets. In this demonstration here we will always first empty the table in the first batch after the application is started. We use Spark's *'Overwrite'* mode for writing the first batch to clear a pre-existing table and then use *'Append'* mode in subsequent batches.\n",
    "\n",
    "You can check in your Db2 Warehouse console that a table called USERDATA.IOT_EVENTS has been created. For example, run the following SQL query to see how many records have been written: \n",
    "> SELECT COUNT(*) FROM USERDATA.IOT_EVENTS.\n",
    "\n",
    "You will notice that there are only a few rows compared to the number of generated records. For example, with a timeout set to 10s, you will collect approximately 8 rows. With a timeout of 50s, there will be approximetely 30 etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "org.apache.kafka.common.config.ConfigException: Missing required configuration \"partition.assignment.strategy\" which has no default value.\n",
      "  at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:124)\n",
      "  at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:48)\n",
      "  at org.apache.kafka.clients.consumer.ConsumerConfig.<init>(ConsumerConfig.java:194)\n",
      "  at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:380)\n",
      "  at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:363)\n",
      "  at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:350)\n",
      "  at org.apache.spark.streaming.kafka010.Subscribe.onStart(ConsumerStrategy.scala:84)\n",
      "  at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.consumer(DirectKafkaInputDStream.scala:70)\n",
      "  at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.start(DirectKafkaInputDStream.scala:240)\n",
      "  at org.apache.spark.streaming.DStreamGraph$$anonfun$start$7.apply(DStreamGraph.scala:54)\n",
      "  at org.apache.spark.streaming.DStreamGraph$$anonfun$start$7.apply(DStreamGraph.scala:54)\n",
      "  at scala.collection.parallel.mutable.ParArray$ParArrayIterator.foreach_quick(ParArray.scala:143)\n",
      "  at scala.collection.parallel.mutable.ParArray$ParArrayIterator.foreach(ParArray.scala:136)\n",
      "  at scala.collection.parallel.ParIterableLike$Foreach.leaf(ParIterableLike.scala:972)\n",
      "  at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)\n",
      "  at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\n",
      "  at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\n",
      "  at scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)\n",
      "  at scala.collection.parallel.ParIterableLike$Foreach.tryLeaf(ParIterableLike.scala:969)\n",
      "  at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)\n",
      "  at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)\n",
      "  at scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)\n",
      "  at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n",
      "  at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n",
      "  at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n",
      "  at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n",
      "  at ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()\n",
      "  at org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:578)\n",
      "  at org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:572)\n",
      "  at runStreamingContext(<console>:62)\n",
      "  ... 52 elided\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val tableName = \"USERDATA.IOT_EVENTS\"\n",
    "\n",
    "var saveMode = SaveMode.Overwrite\n",
    "def processStream2(messages: inputStream): Unit = {\n",
    "    messages.map(_.value).foreachRDD(rdd => {\n",
    "        val ds = spark.read.schema(schema).json(rdd)\n",
    "        val flatDataset = ds.select($\"payload.temperature\", $\"payload.tempOutside\",\n",
    "            $\"payload.powerProd\", $\"payload.noiseLevel1\", $\"payload.time\",\n",
    "            $\"deviceId\", $\"deviceType\", $\"eventType\")\n",
    "        flatDataset.write.format(\"com.ibm.idax.spark.idaxsource\").\n",
    "            option(\"dbtable\", tableName).\n",
    "            option(\"allowAppend\", \"TRUE\").\n",
    "            mode(saveMode).\n",
    "            save()\n",
    "        println(\"Batch written to database\")\n",
    "        saveMode = SaveMode.Append\n",
    "    })\n",
    "}\n",
    "runStreamingContext(processStream2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you want to capture some longer timeframes of events in this notebook, increase the timeoutMillisecs e.g. by running the cell below and then running the previous cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeoutMillisecs: Option[Long] = Some(120000)\n"
     ]
    }
   ],
   "source": [
    "timeoutMillisecs = Some(120000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying automatic background IoT landing app inside dashDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally you can deploy the essential parts of this notebook (i.e. by excluding unnecessary cells with a`//NOT-FOR-APP` comment) as a stand-alone scala application into dashDB. To do this, select `File -> Deploy as -> Deploy to dashDB Spark` from the menu (may not be available, however). Then use one of the alternatives shown in the result page to launch the application, e.g. using the [SPARK_SUBMIT](https://www.ibm.com/support/knowledgecenter/SS6NHC/com.ibm.swg.im.dashdb.analytics.doc/doc/r_spark_applications_spark_submit.html) stored procedure. \n",
    "\n",
    "If you want to take a look at the generated code, select `File -> Download as -> Scala class (in browser)`\n",
    "\n",
    "This notebook is set up so that the timeout does not apply in the exported application, so your streaming context will continue to run and insert events into the database until the application is explicitly stopped via [spark-submit.sh --kill](https://www.ibm.com/support/knowledgecenter/SS6NHC/com.ibm.swg.im.dashdb.doc/learn_how/spark_ref.html), the \n",
    "[CANCEL_APP](https://www.ibm.com/support/knowledgecenter/SS6NHC/com.ibm.swg.im.dashdb.analytics.doc/doc/r_spark_applications_cancel_app.html) stored procedure, the [/public/apps/cancel](https://developer.ibm.com/clouddataservices/wp-content/themes/projectnext-clouddata/dashDBanalytics/#/) endpoint of dashDB's REST API or the Spark monitoring UI in the dashDB console (`Monitor->Workloads->Spark`).\n",
    "\n",
    "**For your convenience** you can simply use the cells in the next section to start and stop the deployed app in the background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launching deployed IoD landing app in background\n",
    "Run the following cell to launch the just deployed IoD landing app inside dashDB. The cell keeps a database connection open that we will use to stop the landing app again when you execute the subsequent cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import java.sql.DriverManager\n",
      "import java.sql.Connection\n",
      "import java.sql.SQLException\n",
      "connection: java.sql.Connection = com.ibm.db2.jcc.uw.UWConnection@27dcabb2\n",
      "iot_lander_submission_id: String = null\n",
      "Error: com.ibm.db2.jcc.am.SqlException: DB2 SQL Error: SQLCODE=-438, SQLSTATE=38A00, SQLERRMC=CDFAA0802E Spark application error: Attempt to submit the application , DRIVER=4.26.14\n"
     ]
    }
   ],
   "source": [
    "import java.sql.DriverManager\n",
    "import java.sql.Connection\n",
    "import java.sql.SQLException\n",
    "val connection = DriverManager.getConnection(\"jdbc:db2:BLUDB\")\n",
    "var iot_lander_submission_id : String = null\n",
    "try {   \n",
    "    val sp_call = connection.prepareCall(\n",
    "      \"CALL IDAX.SPARK_SUBMIT(?, '{ \\\"appResource\\\" : \\\"IoT_demo-assembly-1.0.jar\\\", \" +\n",
    "                                    \"\\\"mainClass\\\" : \\\"SampleApp\\\"}', \" +\n",
    "                                    \"'mode=async')\")\n",
    "    sp_call.registerOutParameter(1, java.sql.Types.VARCHAR);\n",
    "    sp_call.executeUpdate();\n",
    "    iot_lander_submission_id = sp_call.getString(1)\n",
    "    println(\"Successfully launched IoT landing app with submission id \" + iot_lander_submission_id);\n",
    "} catch {\n",
    "    case e: SQLException => { println(\"Error: \" + e) }\n",
    "    iot_lander_submission_id = null\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the cells in the subsequent section to observe the progress of landing IoT messages in the target table. When you are done, you can run the following cell to stop the IoT landing app that you started in the background in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No submission ID defined\n"
     ]
    }
   ],
   "source": [
    "if(iot_lander_submission_id !=null) {\n",
    "    try {   \n",
    "        val sp_call = connection.prepareCall(\"CALL IDAX.CANCEL_APP(?)\")\n",
    "        sp_call.setString(1, iot_lander_submission_id);\n",
    "        sp_call.executeUpdate();\n",
    "        println(\"Successfully stopped IoT landing app with submission id \" + iot_lander_submission_id)\n",
    "    } catch {\n",
    "        case e: SQLException => { println(\"Error: \" + e) }\n",
    "    }\n",
    "} else println(\"No submission ID defined\")\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check landed IoT data\n",
    "You can verify the landed data in Db2 Warehouse. For this we simply establish a data frame on the result table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var landed_iot_data = spark.read.\n",
    "   format(\"com.ibm.idax.spark.idaxsource\").\n",
    "   option(\"url\", \"jdbc:db2:BLUDB\").\n",
    "   option(\"dbtable\", tableName).\n",
    "   load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we show the content. You can verify the progress of your deployed landing app by running the next cell repeatetly and check the new records and overall count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Total IoT records: \"+ landed_iot_data.count())\n",
    "println(\"Newest 10 IoT records:\")\n",
    "landed_iot_data.select(\"deviceId\", \"time\", \"temperature\", \"tempOutside\", \"powerProd\", \"noiseLevel1\").\n",
    "                orderBy(org.apache.spark.sql.functions.col(\"time\").desc).show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Visualisation of time series\n",
    "\n",
    "You could want to visualize the data you are collecting. Let's plot some of the features over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note : Make a bridge between remote and local__\n",
    "\n",
    "So far we've been using Spark servers and the data is accessed remotely. If we plot visualisations now, they won't appear right on your screen : you will have to fetch them, using their adress, to open them locally. So for convenience, visualisations will be made locally thanks to the __%local__ magic. Therefore, we need to create a temporary view of the data, and query it with the %%sql magic in order to access it locally and plot it directly. Note that the data on the Spark cluster is a Spark dataframe, whereas the data we handle locally is a pandas dataframe.\n",
    "\n",
    "In the following cells, we will:\n",
    "1. Create a SQL temporary view of the tornado (Spark) dataframe\n",
    "2. Run a %%sql cell magic to query the data, and output it to the local space. The table is now seen as a Pandas dataframe.\n",
    "3. We use this Pandas dataframe to make our graphs, which we output locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landed_iot_data.createOrReplaceTempView(\"turbine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql -o turbine --maxrows -1\n",
    "SELECT * FROM turbine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%local\n",
    "turbine['time']=pd.to_datetime(turbine['time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have noticed, our data comes from several devices. Let's focus on only one device to start with. You can change the deviceId if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%local\n",
    "print(turbine['deviceId'].unique())\n",
    "device=turbine[turbine['deviceId']==2706]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%local\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "# Basic plot\n",
    "def plot_df(dataFrame, x1, y, title=\"\", xlabel='Date', ylabel='Value', dpi=100):\n",
    "    plt.figure(figsize=(16,5), dpi=dpi)\n",
    "    plt.plot(x1, y, color='tab:red')\n",
    "    plt.gca().set(title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    plt.show()\n",
    "    \n",
    "plot_df(device, x1=device.time, y=device.temperature, title='Temperature over time - sensor 2706')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It could be useful to compare the temperature measured by the sensor with the power production over time. Is the temperature raising over time? Does an increase in power production implies an increase in temperature? With some lag?\n",
    "\n",
    "Note that the data generator we used here doesn't provide very meaningful data regarding Physics... Furthermore, we would need a far longer observation time to make any conclusion. However, the following visualisations are provided for the sake of our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%local\n",
    "\n",
    "# Temperature and power production over time in a particular sensor\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.xlabel('Time')\n",
    "\n",
    "ax1 = device.temperature.plot(color='blue', grid=True, label='Temperature')\n",
    "ax2 = device.powerProd.plot(color='red', grid=True, secondary_y=True, label='PowerProd')\n",
    "\n",
    "h1, l1 = ax1.get_legend_handles_labels()\n",
    "h2, l2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "\n",
    "plt.legend(h1+h2, l1+l2, loc=2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
