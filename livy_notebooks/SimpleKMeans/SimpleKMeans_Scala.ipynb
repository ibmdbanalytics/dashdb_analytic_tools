{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Spark ML on Db2 Warehouse sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import\n",
    "\n",
    "Import the necessary Spark classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>91</td><td>None</td><td>spark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.Pipeline\n",
      "import org.apache.spark.ml.clustering.KMeans\n",
      "import org.apache.spark.ml.clustering.KMeansModel\n",
      "import org.apache.spark.ml.feature.VectorAssembler\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.clustering.KMeans\n",
    "import org.apache.spark.ml.clustering.KMeansModel\n",
    "import org.apache.spark.ml.feature.VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load\n",
    "\n",
    "Load the data from the Db2 Warehouse TRAINING sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: org.apache.spark.sql.DataFrame = [CHURN: smallint, AREA: int ... 15 more fields]\n",
      "[CHURN: smallint, AREA: int ... 15 more fields]\n"
     ]
    }
   ],
   "source": [
    "val data = spark.read.format(\"com.ibm.idax.spark.idaxsource\").\n",
    "    option(\"url\", \"jdbc:db2:BLUDB\").\n",
    "    option(\"dbtable\", \"SAMPLES.TRAINING\").\n",
    "    option(\"mode\", \"JDBC\").\n",
    "    load()\n",
    "println(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is about phone calls and can be used for churn prediction and consumer habits analysis. Columns include whether the customer resigned his contract, the number of minutes for day and evening calls, the corresponding charge, a special category for international calls... In the following section we are going to build customer clusters on the basis of the time they spend on the phone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model\n",
    "\n",
    "Build a Spark ML pipeline that selects the call counts from the customer data and clusters them using KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_31c8ff1f8679\n",
      "clustering: org.apache.spark.ml.clustering.KMeans = kmeans_9e40851c3b54\n",
      "pipe: org.apache.spark.ml.Pipeline = pipeline_c46f66e417f5\n"
     ]
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().\n",
    "    setInputCols(Array(\"INTL_CALLS\", \"DAY_CALLS\", \"EVE_CALLS\", \"NIGHT_CALLS\")).\n",
    "    setOutputCol(\"features\")\n",
    "\n",
    "val clustering = new KMeans().\n",
    "    setFeaturesCol(\"features\").\n",
    "    setK(3).\n",
    "    setMaxIter(3)\n",
    "\n",
    "val pipe = new Pipeline().\n",
    "    setStages(Array(assembler, clustering))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the pipeline to find the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: org.apache.spark.ml.PipelineModel = pipeline_c46f66e417f5\n"
     ]
    }
   ],
   "source": [
    "val model = pipe.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.49812734082397,117.52059925093633,106.0314606741573,101.74756554307116]\n",
      "[4.524608501118568,82.92170022371364,114.31991051454139,106.04474272930649]\n",
      "[4.420289855072464,93.95833333333333,81.45561594202898,93.31702898550725]\n"
     ]
    }
   ],
   "source": [
    "model.stages(1).asInstanceOf[KMeansModel].clusterCenters.foreach { println }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
