{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction with IBM Db2 Warehouse using PySpark\n",
    "\n",
    "# Part 3 : Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Introduction__\n",
    "\n",
    "This notebook presents a churn prediction use case using anonymized customer data from a phone operator. It uses IBM Db2 Warehouse and runs on a PySpark kernel. It is the third part of a series on this use case. It is focused on deployment: in the previous notebook, we had saved our models. Let's reuse them on some fresh data!\n",
    "\n",
    "__Use case__\n",
    "\n",
    "Our goal is to accurately predict whether a customer is going to end his/her contract (labeled as positive,1). We prefer to send a commercial email to someone who intends to keep her contract but is labeled as willing to end it (false positive) rather than to overlook the opportunity of preventing a customer from ending her contract (false negative). We also care to accurately target customers with engagement campaigns : not overwhelming customers with commercials and not losing money by proposing special offers to too many people (precision and accuracy). Our optimization objective thus consisted in maximizing recall i.e. minimizing the false negative rate. We also looked at a couple of other indicators such as accuracy and area under curve.\n",
    "\n",
    "\n",
    "__Previously__\n",
    "\n",
    "In the first notebook, we used PySpark for data exploration and visualization. We created, scaled and selected features. In the second notebook, we built and tested several models. We selected the model with the highest recall on the test set. \n",
    "\n",
    "__Contents__\n",
    "1. Get ready\n",
    "2. Load fresh data\n",
    "3. Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Imports__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>205</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Basics\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Classification models\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.ml.classification import GBTClassificationModel\n",
    "from pyspark.ml.clustering import KMeansModel\n",
    "from pyspark.ml.feature import MinMaxScalerModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Open the models__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# note : please modify the path if you did not save the previous notebook in the same folder as this one\n",
    "rfModel = RandomForestClassificationModel.load(\"/tmp/myRFModel\")\n",
    "clusterModel = KMeansModel.load(\"/tmp/myClusterModel\")\n",
    "scalerModel = MinMaxScalerModel.load(\"/tmp/myScalerModel\")\n",
    "\n",
    "# if you want to use other models for comparison\n",
    "#gbtModel = GBTClassificationModel.load(\"/tmp/myGBTModel\")\n",
    "#lrModel = LogisticRegressionModel.load(\"/tmp/myLogRegModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|ClusterID|           AVG_CHURN|\n",
      "+---------+--------------------+\n",
      "|        8|   0.563953488372093|\n",
      "|        6| 0.09444444444444444|\n",
      "|       11| 0.11805555555555555|\n",
      "|        5| 0.06030150753768844|\n",
      "|       10|0.061068702290076333|\n",
      "|        2| 0.08121827411167512|\n",
      "|        4| 0.08292682926829269|\n",
      "|       13|0.025477707006369428|\n",
      "|        1| 0.02564102564102564|\n",
      "|       12| 0.07142857142857142|\n",
      "|        3|  0.0728476821192053|\n",
      "|        0|0.048484848484848485|\n",
      "|       14| 0.05333333333333334|\n",
      "|        7| 0.24861878453038674|\n",
      "|        9|  0.4178082191780822|\n",
      "+---------+--------------------+"
     ]
    }
   ],
   "source": [
    "# open the CHURN_PROPORTION table you had previously saved\n",
    "churn_proportion = spark.read \\\n",
    "        .format(\"com.ibm.idax.spark.idaxsource\") \\\n",
    "        .options(dbtable=\"AVG_CLUSTER_CHURN\") \\\n",
    "        .load()\n",
    "\n",
    "churn_proportion.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load fresh data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Open the data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+--------+---------+----------+--------+---------+----------+----------+-----------+------------+---------+----------+-----------+---------+\n",
      "|AREA|VMAIL|VMAIL_MSGS|DAY_MINS|DAY_CALLS|DAY_CHARGE|EVE_MINS|EVE_CALLS|EVE_CHARGE|NIGHT_MINS|NIGHT_CALLS|NIGHT_CHARGE|INTL_MINS|INTL_CALLS|INTL_CHARGE|SVC_CALLS|\n",
      "+----+-----+----------+--------+---------+----------+--------+---------+----------+----------+-----------+------------+---------+----------+-----------+---------+\n",
      "| 415|    0|        25|   265.1|      110|     45.07|  197.40|       99|     16.78|     244.7|         91|       11.01|     10.0|         3|       2.70|        1|\n",
      "+----+-----+----------+--------+---------+----------+--------+---------+----------+----------+-----------+------------+---------+----------+-----------+---------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "# a table has been prepopulated in Db2 with a sample of unlabeled customer data : SAMPLES.EVAL\n",
    "sparkSession = spark \\\n",
    "        .builder \\\n",
    "        .getOrCreate()\n",
    "\n",
    "df = spark.read \\\n",
    "        .format(\"com.ibm.idax.spark.idaxsource\") \\\n",
    "        .options(dbtable=\"SAMPLES.EVAL\") \\\n",
    "        .load()\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows 4"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows \"+str(df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 4 new examples. They are not labeled. It's up to us to make predictions!\n",
    "\n",
    "But first, let's transform our data into a suitable format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Prepare the data__\n",
    "\n",
    "1. Add new columns\n",
    "2. Scale the numerical features\n",
    "2. Add the ClusterChurn feature\n",
    "3. Assemble features for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 1. Add new columns\n",
    "\n",
    "# 4 new features\n",
    "TOT_MINS = df['DAY_MINS']+df['EVE_MINS']+df['INTL_MINS']+df['NIGHT_MINS']\n",
    "DAY_MINS_perCALL = df['DAY_MINS']/df['DAY_CALLS']\n",
    "NIGHT_MINS_perCALL = df['NIGHT_MINS']/df['NIGHT_CALLS']\n",
    "EVE_MINS_perCALL = df['EVE_MINS']/df['EVE_CALLS']\n",
    "\n",
    "# Add the columns to the existing ones in a new dataframe\n",
    "tot_df = df.withColumn(\"TOT_MINS\", TOT_MINS).withColumn(\"DAY_MINS_perCALL\", DAY_MINS_perCALL).withColumn(\"NIGHT_MINS_perCALL\", NIGHT_MINS_perCALL).withColumn(\"EVE_MINS_perCALL\", EVE_MINS_perCALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 2. Scale the numerical features\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"TOT_MINS\", \"DAY_MINS_perCALL\", \"EVE_MINS_perCALL\", \"NIGHT_MINS_perCALL\", \"VMAIL_MSGS\", \"INTL_CALLS\", \"DAY_CALLS\", \"EVE_CALLS\", \"NIGHT_CALLS\", \"SVC_CALLS\", \"INTL_CHARGE\", \"DAY_CHARGE\", \"EVE_CHARGE\", \"NIGHT_CHARGE\"],\n",
    "    outputCol=\"rawFeatures\")\n",
    "assembled_df = assembler.transform(tot_df)\n",
    "\n",
    "scaled_df = scalerModel.transform(assembled_df)\n",
    "\n",
    "#Unroll scaled features vector and reinsert CHURN and VMAIL columns\n",
    "columns = [\"VMAIL\", \"TOT_MINS\", \"DAY_MINS_perCALL\", \"EVE_MINS_perCALL\", \"NIGHT_MINS_perCALL\", \"VMAIL_MSGS\", \"INTL_CALLS\", \"DAY_CALLS\", \"EVE_CALLS\", \"NIGHT_CALLS\", \"SVC_CALLS\", \"INTL_CHARGE\", \"DAY_CHARGE\", \"EVE_CHARGE\", \"NIGHT_CHARGE\"]\n",
    "\n",
    "full_scaled_df = scaled_df.rdd.map(lambda x:[x[\"VMAIL\"]]+[float(y) for y in x['features']]).toDF(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 3. Add the ClusterChurn feature (i.e. AVG_CHURN)\n",
    "\n",
    "# Assembler for kmeans\n",
    "assembler_16 = VectorAssembler(\n",
    "    inputCols=['VMAIL', 'TOT_MINS', 'DAY_MINS_perCALL', 'EVE_MINS_perCALL', \n",
    "               'NIGHT_MINS_perCALL', 'VMAIL_MSGS', 'INTL_CALLS', 'DAY_CALLS', \n",
    "               'EVE_CALLS', 'NIGHT_CALLS', 'SVC_CALLS', 'INTL_CHARGE', \n",
    "               'DAY_CHARGE', 'EVE_CHARGE', 'NIGHT_CHARGE'],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "# Join tables on ClusterID to add a new column\n",
    "def preparation(DF):\n",
    "    \n",
    "    # assemble \n",
    "    DF_16 = assembler_16.transform(DF)\n",
    "    \n",
    "    # Assign each point of the training set to its cluster\n",
    "    DF_prediction = clusterModel.transform(DF_16)\n",
    "    \n",
    "    # Join DF with table churn_proportion on ClusterID\n",
    "    DF_joined = DF_prediction.join(churn_proportion, DF_prediction.prediction == churn_proportion.ClusterID, \"inner\")\n",
    "    #DF_joined.show(1)\n",
    "    #DF_joined.printSchema()\n",
    "    \n",
    "    # Rename columns\n",
    "    DF_prepared = DF_joined.withColumnRenamed(\"features\", \"featuresClustering\").withColumnRenamed(\"prediction\", \"predictionClustering\").withColumnRenamed(\"AVG_CHURN\", \"ClusterChurn\")\n",
    "    #DF_prepared.printSchema()\n",
    "    \n",
    "    return DF_prepared\n",
    "\n",
    "prep_df = preparation(full_scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VMAIL: long (nullable = true)\n",
      " |-- TOT_MINS: double (nullable = true)\n",
      " |-- DAY_MINS_perCALL: double (nullable = true)\n",
      " |-- EVE_MINS_perCALL: double (nullable = true)\n",
      " |-- NIGHT_MINS_perCALL: double (nullable = true)\n",
      " |-- VMAIL_MSGS: double (nullable = true)\n",
      " |-- INTL_CALLS: double (nullable = true)\n",
      " |-- DAY_CALLS: double (nullable = true)\n",
      " |-- EVE_CALLS: double (nullable = true)\n",
      " |-- NIGHT_CALLS: double (nullable = true)\n",
      " |-- SVC_CALLS: double (nullable = true)\n",
      " |-- INTL_CHARGE: double (nullable = true)\n",
      " |-- DAY_CHARGE: double (nullable = true)\n",
      " |-- EVE_CHARGE: double (nullable = true)\n",
      " |-- NIGHT_CHARGE: double (nullable = true)\n",
      " |-- featuresClustering: vector (nullable = true)\n",
      " |-- predictionClustering: integer (nullable = false)\n",
      " |-- ClusterID: integer (nullable = false)\n",
      " |-- ClusterChurn: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "prep_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+-------------------+-------------------+-------------------+----------+------------------+-------------------+-------------------+------------------+-------------------+-------------------+------------------+------------------+--------------------+--------------------+---------+--------------------+\n",
      "|VMAIL|          TOT_MINS|  DAY_MINS_perCALL|   EVE_MINS_perCALL| NIGHT_MINS_perCALL|         VMAIL_MSGS|INTL_CALLS|         DAY_CALLS|          EVE_CALLS|        NIGHT_CALLS|         SVC_CALLS|        INTL_CHARGE|         DAY_CHARGE|        EVE_CHARGE|      NIGHT_CHARGE|  featuresClustering|predictionClustering|ClusterID|        ClusterChurn|\n",
      "+-----+------------------+------------------+-------------------+-------------------+-------------------+----------+------------------+-------------------+-------------------+------------------+-------------------+-------------------+------------------+------------------+--------------------+--------------------+---------+--------------------+\n",
      "|    1|0.4907607790910604|0.1393043325787072| 0.1763809107224469|0.27790576164916614|                0.0|       0.3|0.8404907975460123|0.48823529411764705|  0.539568345323741|0.4444444444444444| 0.6351851851851852|0.37228629579375844|0.6449684490202591|0.4997011356843992|[1.0,0.4907607790...|                   9|        9|  0.4178082191780822|\n",
      "|    0|0.7206592308972865|0.3562673236834176|0.12774838615735884|0.41368484327623345|0.49019607843137253|      0.15|0.6748466257668712| 0.5823529411764706|0.39568345323741005|0.1111111111111111|                0.5| 0.7644165535956581|0.5572899368980405|0.5959354453078302|[0.0,0.7206592308...|                  10|       10|0.061068702290076333|\n",
      "|    0|0.5675045779923423|0.1942205563108442|0.12160544062534376| 0.3768619789444366| 0.5098039215686274|      0.15| 0.754601226993865| 0.6058823529411764|0.48201438848920863|0.1111111111111111| 0.6851851851851851| 0.4659090909090909|0.5519760876785121|0.6222355050806934|[0.0,0.5675045779...|                  11|       11| 0.11805555555555555|\n",
      "|    1|  0.88629931746296|0.7345103861659247|0.20990637435945778|0.17263531140159707|                0.0|      0.45|0.4110429447852761| 0.5705882352941176| 0.6618705035971223|0.4444444444444444|0.27037037037037037| 0.9598032564450475|0.8970441713716374| 0.369994022713688|[1.0,0.8862993174...|                   8|        8|   0.563953488372093|\n",
      "+-----+------------------+------------------+-------------------+-------------------+-------------------+----------+------------------+-------------------+-------------------+------------------+-------------------+-------------------+------------------+------------------+--------------------+--------------------+---------+--------------------+"
     ]
    }
   ],
   "source": [
    "prep_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 4. Assemble features for supervised learning\n",
    "\n",
    "assembler_12 = VectorAssembler(\n",
    "    inputCols=['VMAIL', 'TOT_MINS', 'DAY_MINS_perCALL', 'EVE_MINS_perCALL', \n",
    "               'NIGHT_MINS_perCALL', 'VMAIL_MSGS', 'INTL_CALLS', 'DAY_CALLS', \n",
    "               'EVE_CALLS', 'NIGHT_CALLS', 'SVC_CALLS', 'INTL_CHARGE', \n",
    "               'DAY_CHARGE', 'EVE_CHARGE', 'NIGHT_CHARGE', 'ClusterChurn'],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "assembled_df = assembler_12.transform(prep_df)\n",
    "\n",
    "# Select only the features columns\n",
    "labeled_df = assembled_df.select(assembled_df[\"features\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Prediction__\n",
    "\n",
    "Let's see what predictions are made by our model on this unseen data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|[1.0,0.4907607790...|[9.40259740259740...|[0.37610389610389...|       1.0|\n",
      "|[0.0,0.7206592308...|[23.8575492636634...|[0.95430197054653...|       0.0|\n",
      "|[0.0,0.5675045779...|[24.0624543699248...|[0.96249817479699...|       0.0|\n",
      "|[1.0,0.8862993174...|          [3.0,22.0]|         [0.12,0.88]|       1.0|\n",
      "+--------------------+--------------------+--------------------+----------+"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "rf_pred = rfModel.transform(labeled_df)\n",
    "\n",
    "print(\"Random Forest\")\n",
    "rf_pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final prediction vector is [1,0,0,1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you've learned\n",
    "\n",
    "Congratulations!\n",
    "\n",
    "In this notebook, you've seen how to:\n",
    "* load models into a Jupyter notebook\n",
    "* load data you had saved in Db2\n",
    "* deploy models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## Authors\n",
    "\n",
    "Eva Feillet - ML intern, IBM Cloud and Cognitive Software, IBM Lab in Böblingen, Germany"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
